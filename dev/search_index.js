var documenterSearchIndex = {"docs":
[{"location":"api/#Public-API","page":"API Reference","title":"Public API","text":"","category":"section"},{"location":"api/#KeemenaPreprocessing.TOKENIZERS","page":"API Reference","title":"KeemenaPreprocessing.TOKENIZERS","text":"TOKENIZERS\n\nA constant Tuple{Symbol} listing the names of built-in tokenizers that can be passed to the tokenizer_name keyword of PreprocessConfiguration.\n\nCurrently supported values are\n\n:whitespace - split on Unicode whitespace;\n:unicode    - iterate user-perceived graphemes (eachgrapheme);\n:byte       - treat the text as raw bytes (byte-level models);\n:char       - split on individual UTF-8 code units.\n\nYou may also supply any callable that implements mytokens = f(string) in place of one of these symbols.\n\n\n\n\n\n","category":"constant"},{"location":"api/#KeemenaPreprocessing.Corpus","page":"API Reference","title":"KeemenaPreprocessing.Corpus","text":"Corpus\n\nFlat, memory-efficient container that stores an entire corpus of token-ids together with optional hierarchical offset tables that recover the original structure (documents â†’ paragraphs â†’ sentences â†’ words â†’ characters â†’ bytes).\n\nEvery offset vector records the starting index (1-based, inclusive) of each unit inside token_ids.  The final entry therefore equals length(token_ids)+1, making range retrieval convenient via view(token_ids, offsets[i] : offsets[i+1]-1).\n\nFields\n\nfield type always present? description\ntoken_ids Vector{Int} âœ“ Concatenated token identifiers returned by the vocabulary.\ndocument_offsets Vector{Int} âœ“ Start positions of each document (outermost level).\nparagraph_offsets Union{Vector{Int},Nothing} cfg-dependent Paragraph starts within each document when record_paragraph_offsets=true.\nsentence_offsets Union{Vector{Int},Nothing} cfg-dependent Sentence boundaries when record_sentence_offsets=true.\nword_offsets Union{Vector{Int},Nothing} cfg-dependent Word boundaries when record_word_offsets=true.\ncharacter_offsets Union{Vector{Int},Nothing} cfg-dependent Unicode-character spans when record_character_offsets=true.\nbyte_offsets Union{Vector{Int},Nothing} cfg-dependent Byte-level spans when record_byte_offsets=true.\n\nExample\n\n# assume `corp` is a Corpus produced by preprocess_corpus\ndoc1_range = corp.document_offsets[1] : corp.document_offsets[2]-1\ndoc1_token_ids = view(corp.token_ids, doc1_range)\n\nif corp.sentence_offsets â‰  nothing\n    first_sentence = view(corp.token_ids,\n                          corp.sentence_offsets[1] : corp.sentence_offsets[2]-1)\nend\n\nThe presence or absence of each optional offsets vector is determined entirely by the corresponding record_*_offsets flags in PreprocessConfiguration.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.CrossMap","page":"API Reference","title":"KeemenaPreprocessing.CrossMap","text":"CrossMap\n\nAlignment table that links two segmentation levels of the same corpus (e.g. bytes -> characters, characters -> words, words -> sentences).\n\nFor every unit in the destination level the alignment vector stores the 1-based index into the source offsets at which that unit begins. This allows constant-time projection of any span expressed in destination units back to the finer-grained source sequence.\n\nFields\n\nsource_level      :: Symbol   Name of the finer level (must match a key in bundle.levels, typically :byte, :char, :word, :sentence, or :paragraph).\ndestination_level :: Symbol   Name of the coarser level whose boundaries are encoded.\nalignment         :: Vector{Int}   Length = N_destination + 1.   alignment[i] is the starting source-level offset of destination element i; the extra sentinel entry alignment[end] = N_source + 1 lets you slice with   alignment[i] : alignment[i+1]-1 without bounds checks.\n\nExample\n\n# map words â‡’ sentences\nm = CrossMap(:word, :sentence, sent2word_offsets)\n\nfirst_sentence_word_ids = alignment_view(m, 1)  # helper returning a view\n\nThe constructor is trivial and performs no validation; pipelines are expected to guarantee consistency when emitting CrossMap objects.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.CrossMap-Tuple{Symbol, Symbol, AbstractVector{<:Integer}}","page":"API Reference","title":"KeemenaPreprocessing.CrossMap","text":"CrossMap(src, dst, align)\n\nShorthand outer constructor that builds a CrossMap while materialising the alignment vector as Vector{Int}.\n\nArguments\n\nsrc::Symbol - identifier of the source (finer-grained) level   (e.g. :char, :word).\ndst::Symbol - identifier of the destination (coarser) level   (e.g. :word, :sentence).\nalign::AbstractVector{<:Integer} - offset array mapping every destination unit to its starting position in the source sequence.  Any integer-typed vector is accepted; it is copied into a dense Vector{Int} to guarantee contiguous storage and type stability inside the resulting CrossMap.\n\nReturns\n\nA CrossMap(src, dst, Vector{Int}(align)).\n\nExample\n\ncm = CrossMap(:char, :word, UInt32[1, 5, 9, 14])\n@assert cm.alignment isa Vector{Int}\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.LevelBundle","page":"API Reference","title":"KeemenaPreprocessing.LevelBundle","text":"LevelBundle\n\nSelf-contained pairing of a Corpus and its companion Vocabulary.  A LevelBundle represents one segmentation level (e.g. words, characters, or bytes) produced by the preprocessing pipeline. By storing both objects side-by-side it guarantees that every token_id found in corpus.token_ids is valid according to vocabulary.\n\nFields\n\ncorpus     :: Corpus   All token-ids plus optional offset tables describing the structure of the text at this level.\nvocabulary :: Vocabulary   Bidirectional mapping between token strings and the integer ids used in corpus.token_ids.\n\nIntegrity checks\n\nThe inner constructor performs two runtime validations:\n\nRange check - the largest token-id must not exceed length(vocabulary.id_to_token_strings).\nLower bound - all token-ids must be >= 1 (id 0 is never legal).\n\nViolations raise an informative ArgumentError, catching mismatches early.\n\nExample\n\nword_corpus  = Corpus(word_ids, doc_offs, nothing, sent_offs, word_offs,\n                      nothing, nothing)\nword_vocab   = build_vocabulary(words; minimum_token_frequency = 2)\n\nword_bundle  = LevelBundle(word_corpus, word_vocab)\n\nnb_tokens    = length(word_bundle.vocabulary.id_to_token_strings)\n@info \"bundle contains nb_tokens unique tokens\"\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.PipelineMetadata","page":"API Reference","title":"KeemenaPreprocessing.PipelineMetadata","text":"PipelineMetadata\n\nCompact header bundled with every artefact produced by KeemenaPreprocessing. It records the exact pipeline settings and the version of the on-disk schema so that data can be re-processed, inspected, or migrated safely.\n\nFields\n\nconfiguration::PreprocessConfiguration   The full set of cleaning, tokenisation, vocabulary, and offset-recording options that generated the artefact.  Storing this ensures strict reproducibility.\nschema_version::VersionNumber   The version of the bundle file format (not the Julia package).   Increment the major component when breaking changes are introduced so that loaders can detect incompatibilities and perform migrations or raise errors.\n\nExample\n\ncfg  = PreprocessConfiguration(strip_html_tags = true)\nmeta = PipelineMetadata(cfg, v\"1.0.0\")\n\n@info \"tokeniser:\" meta.configuration.tokenizer_name\n@assert meta.schema_version >= v\"1.0.0\"\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.PipelineMetadata-Tuple{}","page":"API Reference","title":"KeemenaPreprocessing.PipelineMetadata","text":"PipelineMetadata() -> PipelineMetadata\n\nConvenience constructor that returns a metadata header with\n\nthe default PreprocessConfiguration() (all keyword-arguments left at their documented defaults); and\nthe current bundle schema version v\"1.0.0\".\n\nHandy for rapid prototyping or unit tests when you do not need to customise the pipeline but still require a valid PipelineMetadata object.\n\nIdentical to:\n\nPipelineMetadata(PreprocessConfiguration(), v\"1.0.0\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.PreprocessBundle","page":"API Reference","title":"KeemenaPreprocessing.PreprocessBundle","text":"PreprocessBundle{ExtraT}\n\nTop-level artefact emitted by preprocess_corpus (or the streaming variant). A bundle contains everything required to feed a downstream model or to reload a corpus without re-running the expensive preprocessing pipeline.\n\nType parameter\n\nExtraT - arbitrary payload for user-defined information (e.g. feature matrices, clustering assignments, language tags).  Use Nothing when no extras are needed.\n\nFields\n\nfield type description\nlevels Dict{Symbol,LevelBundle} Mapping from segmentation level name (:byte, :char, :word, :sentence, :paragraph, â€¦) to the corresponding LevelBundle.\nmetadata PipelineMetadata Reproducibility header (configuration + schema version).\nalignments Dict{Tuple{Symbol,Symbol},CrossMap} Pair-wise offset projections between levels, keyed as (source, destination) (e.g. (:char, :word)).\nextras ExtraT Optional user payload carried alongside the core data.\n\nTypical workflow\n\nbund = preprocess_corpus(files; strip_html_tags=true)\n\n# inspect vocabulary\nword_vocab = bund.levels[:word].vocabulary\nprintln(\"vocabulary size: \", length(word_vocab.id_to_token_strings))\n\n# project a sentence span back to character offsets\ncm = bund.alignments[(:char, :sentence)]\nfirst_sentence_char_span = cm.alignment[1] : cm.alignment[2]-1\n\nThe bundle is immutable; to add additional levels or extras create a fresh instance (helper functions add_level!, with_extras, etc. are provided by the package).\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.PreprocessBundle-Tuple{Dict{Symbol, <:LevelBundle}}","page":"API Reference","title":"KeemenaPreprocessing.PreprocessBundle","text":"PreprocessBundle(levels; metadata = PipelineMetadata(),\n                      alignments = Dict{Tuple{Symbol,Symbol},CrossMap}(),\n                      extras = nothing) -> PreprocessBundle\n\nOuter constructor that validates and assembles the individual artefacts generated by KeemenaPreprocessing into a single PreprocessBundle.\n\nRequired argument\n\nlevels::Dict{Symbol,<:LevelBundle} - at least one segmentation level (keyed by level name such as :word or :char).\n\nOptional keyword arguments\n\nkeyword default purpose\nmetadata PipelineMetadata() Configuration & schema header.\nalignments empty Dict Maps (source,destination) -> CrossMap.\nextras nothing User-supplied payload propagated unchanged.\n\nRuntime checks\n\nNon-empty levels.\nFor each (lvl, lb) in levels run validate_offsets(lb.corpus, lvl) to ensure internal offset consistency.\nFor every supplied alignment (src,dst) â†’ cm:\nboth src and dst must exist in levels;\nlength(cm.alignment) == length(levels[src].corpus.token_ids);\ncm.source_level      == src;\ncm.destination_level == dst.\n\nAny violation throws an informative ArgumentError.\n\nReturns\n\nA fully-validated PreprocessBundle{typeof(extras)} containing: Dict(levels), metadata, Dict(alignments), and extras.\n\nExample\n\nword_bundle = LevelBundle(word_corpus, word_vocab)\nchar_bundle = LevelBundle(char_corpus, char_vocab)\n\nbund = PreprocessBundle(Dict(:word=>word_bundle, :char=>char_bundle);\n                        alignments = Dict((:char,:word)=>char2word_map))\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.PreprocessBundle-Tuple{}","page":"API Reference","title":"KeemenaPreprocessing.PreprocessBundle","text":"PreprocessBundle(; metadata = PipelineMetadata(), extras = nothing) -> PreprocessBundle\n\nConvenience constructor that produces an empty PreprocessBundle:\n\nlevels     = Dict{Symbol,LevelBundle}()  \nalignments = Dict{Tuple{Symbol,Symbol},CrossMap}()  \nmetadata   = metadata (defaults to PipelineMetadata())  \nextras     = extras   (defaults to nothing)\n\nUseful when you want to build a bundle incrementallyâ€”for example, loading individual levels from disk or generating them in separate jobs: while still attaching a common metadata header or arbitrary user payload.\n\nbund = PreprocessBundle()                      # blank skeleton\nbund = merge(bund, load_word_level(\"word.jld\"))  # pseudo-code for adding data\n\nThe returned object's type parameter is inferred from extras so that any payload, including complex structs, can be stored without further boilerplate.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.PreprocessConfiguration-Tuple{}","page":"API Reference","title":"KeemenaPreprocessing.PreprocessConfiguration","text":"PreprocessConfiguration(; kwargs...) -> PreprocessConfiguration\n\nCreate a fully-specified preprocessing configuration.\n\nAll keyword arguments are optional; sensible defaults are provided so that cfg = PreprocessConfiguration() already yields a working pipeline.  Options are grouped below by the stage they affect.\n\nCleaning stage toggles\n\nkeyword default purpose\nlowercase true Convert letters to lower-case.\nstrip_accents true Remove combining accent marks.\nremove_control_characters true Drop Unicode Cc/Cf code-points.\nremove_punctuation true Strip punctuation & symbol characters.\nnormalise_whitespace true Collapse consecutive whitespace.\nremove_zero_width_chars true Remove zero-width joiners, etc.\npreserve_newlines true Keep explicit line breaks.\ncollapse_spaces true Collapse runs of spaces/tabs.\ntrim_edges true Strip leading/trailing whitespace.\n\nURL, e-mail & numbers\n\nkeyword default purpose\nreplace_urls true Replace URLs with url_sentinel.\nreplace_emails true Replace e-mails with mail_sentinel.\nkeep_url_scheme false Preserve http:// / https:// prefix.\nurl_sentinel \"<URL>\" Token inserted for each URL.\nmail_sentinel \"<EMAIL>\" Token inserted for each e-mail.\nreplace_numbers false Replace numbers with number_sentinel.\nnumber_sentinel \"<NUM>\" Token used when replacing numbers.\nkeep_number_decimal false Preserve decimal part.\nkeep_number_sign false Preserve Â± sign.\nkeep_number_commas false Preserve thousands separators.\n\nMark-up & HTML\n\nkeyword default purpose\nstrip_markdown false Remove Markdown formatting.\npreserve_md_code true Keep fenced/inline code while stripping.\nstrip_html_tags false Remove HTML/XML tags.\nhtml_entity_decode true Decode &amp;, &quot;, etc.\n\nEmoji & Unicode\n\nkeyword default purpose\nemoji_handling :keep :keep, :remove, or :sentinel.\nemoji_sentinel \"<EMOJI>\" Used when emoji_handling == :sentinel.\nsqueeze_repeat_chars false Limit repeated character runs.\nmax_char_run 3 Maximum run length when squeezing.\nmap_confusables false Map visually-confusable chars.\nunicode_normalisation_form :none :NFC, :NFD, :NFKC, :NFKD, or :none.\nmap_unicode_punctuation false Replace Unicode punctuation with ASCII.\n\nTokenisation\n\nkeyword default purpose\ntokenizer_name :whitespace One of TOKENIZERS or a callable.\npreserve_empty_tokens false Keep zero-length tokens.\n\nVocabulary construction\n\nkeyword default purpose\nminimum_token_frequency 1 Discard rarer tokens / map to <UNK>.\nspecial_tokens Dict(:unk=>\"<UNK>\", :pad=>\"<PAD>\") Role â‡’ literal mapping.\n\nOffset recording\n\nkeyword default purpose\nrecord_byte_offsets false Record byte-level spans.\nrecord_character_offsets false Record Unicode-char offsets.\nrecord_word_offsets true Record word offsets.\nrecord_sentence_offsets true Record sentence offsets.\nrecord_paragraph_offsets false Record paragraph offsets (forces preserve_newlines = true).\nrecord_document_offsets true Record document offsets.\n\nReturns\n\nA fully-initialised PreprocessConfiguration instance.  Invalid combinations raise AssertionError (e.g. unsupported tokenizer) and certain settings emit warnings when they imply other flags (e.g. paragraph offsets -> preserve_newlines).\n\nSee also: TOKENIZERS and byte_cfg for a pre-canned byte-level configuration.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.Vocabulary","page":"API Reference","title":"KeemenaPreprocessing.Vocabulary","text":"Vocabulary\n\nImmutable lookup table produced by build_vocabulary that maps between integer token-ids and the string literals that appear in a corpus.\n\nFields\n\nid_to_token_strings::Vector{String}   Position i holds the canonical surface form of token-id i (vocab.id_to_token_strings[id] â†’ \"word\").\ntoken_to_id_map::Dict{String,Int}   Fast reverse mapping from token string to its integer id (vocab.token_to_id_map[\"word\"] â†’ id).   Look-ups fall back to the <UNK> id when the string is absent.\ntoken_frequencies::Vector{Int}   Corpus counts aligned with id_to_token_strings (token_frequencies[id] gives the raw frequency of that token).\nspecial_tokens::Dict{Symbol,Int}   Set of reserved ids for sentinel symbols such as :unk, :pad, :bos, :eos, â€¦   Keys are roles (Symbol); values are the corresponding integer ids.\n\nUsage example\n\nvocab = build_vocabulary(tokens; minimum_token_frequency = 3)\n\n@info \"UNK id:    \" vocab.special_tokens[:unk]\n@info \"Â«helloÂ» id:\" vocab.token_to_id_map[\"hello\"]\n@info \"id â†’ token:\" vocab.id_to_token_strings[42]\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.add_level!-Tuple{PreprocessBundle, Symbol, LevelBundle}","page":"API Reference","title":"KeemenaPreprocessing.add_level!","text":"add_level!(bundle, level, lb) -> PreprocessBundle\n\nMutating helper that inserts a new LevelBundle lb into bundle.levels under key level.  The routine:\n\nGuards against duplicates - throws an error if level already exists.  \nValidates the offsets inside lb.corpus for consistency with the supplied level via validate_offsets.  \nStores the bundle and returns the same bundle instance so the call can be chained.\n\nbe aware that, add_level! modifies its first argument in place; if you require     an immutable bundle keep a copy before calling\n\nArguments\n\nname type description\nbundle PreprocessBundle Target bundle to extend.\nlevel Symbol Identifier for the new segmentation level (e.g. :char, :word).\nlb LevelBundle Data + vocabulary for that level.\n\nReturns\n\nThe same bundle, now containing level => lb.\n\nErrors\n\nArgumentError if a level with the same name already exists.\nPropagates any error raised by validate_offsets when lb.corpus is inconsistent.\n\nExample\n\nchar_bundle = LevelBundle(char_corp, char_vocab)\nadd_level!(bund, :character, char_bundle)\n\n@assert has_level(bund, :character)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.byte_cfg-Tuple{}","page":"API Reference","title":"KeemenaPreprocessing.byte_cfg","text":"byte_cfg(; kwargs...) -> PreprocessConfiguration\n\nShorthand constructor that returns a PreprocessConfiguration pre-configured for byte-level tokenisation.\n\nThe wrapper fixes the following fields\n\ntokenizer_name = :byte\nrecord_byte_offsets      = true\nrecord_character_offsets = false\nrecord_word_offsets      = false\n\nwhile forwarding every other keyword argument to PreprocessConfiguration. Use it when building byte-level language-model corpora but still needing the full flexibility to tweak cleaning, vocabulary, or segmentation options:\n\ncfg = byte_cfg(strip_html_tags = true,\n               minimum_token_frequency = 5)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.get_corpus-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.get_corpus","text":"get_corpus(bundle, level) -> Corpus\n\nRetrieve the Corpus object for segmentation level level from a PreprocessBundle.\n\nThis is equivalent to get_level(bundle, level).corpus and is provided as a convenience helper when you only need the sequence of token-ids and offset tables rather than the whole LevelBundle.\n\nArguments\n\nbundle::PreprocessBundle - bundle produced by preprocess_corpus.\nlevel::Symbol - level identifier such as :byte, :word, :sentence, ...\n\nReturns\n\nThe Corpus stored in the requested level.\n\nErrors\n\nThrows an ArgumentError if the level is not present in bundle (see get_level for details).\n\nExample\n\nword_corp = get_corpus(bund, :word)\n\n# iterate over sentences\nsent_offs = word_corp.sentence_offsets\nfor i in 1:length(sent_offs)-1\n    rng = sent_offs[i] : sent_offs[i+1]-1\n    println(view(word_corp.token_ids, rng))\nend\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.get_level-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.get_level","text":"get_level(bundle, level) â†’ LevelBundle\n\nFetch the LevelBundle associated with segmentation level level from a PreprocessBundle.\n\nArguments\n\nbundle::PreprocessBundle â€” bundle returned by preprocess_corpus.\nlevel::Symbol â€” identifier such as :byte, :word, :sentence, ...\n\nReturns\n\nThe requested LevelBundle.\n\nErrors\n\nThrows an ArgumentError when the level is absent, listing all available levels to aid debugging.\n\nExample\n\nword_bundle = get_level(bund, :word)\nprintln(\"vocabulary size: \", length(word_bundle.vocabulary.id_to_token_strings))\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.get_token_ids-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.get_token_ids","text":"get_token_ids(bundle, level) -> Vector{Int}\n\nReturn the vector of token-ids for segmentation level level contained in a PreprocessBundle.\n\nIdentical to   get_corpus(bundle, level).token_ids,   but provided as a convenience helper when you only need the raw id sequence and not the full Corpus object.\n\nArguments\n\nbundle::PreprocessBundle - bundle produced by preprocess_corpus.\nlevel::Symbol - segmentation level identifier (e.g. :byte, :word).\n\nReturns\n\nA Vector{Int} whose length equals the number of tokens at that level.\n\nErrors\n\nThrows an ArgumentError if the requested level is absent (see get_level for details).\n\nExample\n\nword_ids = get_token_ids(bund, :word)\nprintln(\"first ten ids: \", word_ids[1:10])\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.get_vocabulary-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.get_vocabulary","text":"get_vocabulary(bundle, level) -> Vocabulary\n\nReturn the Vocabulary associated with segmentation level level (eg :byte, :word, :sentence) from a given PreprocessBundle\n\nEffectively a shorthand for   get_level(bundle, level).vocabulary\n\nArguments\n\nbundle::PreprocessBundle - Bundle produced by preprocess_corpus\nlevel::Symbol â€” Level identifier whose vocabulary you need\n\nReturns\n\nThe Vocabulary stored for level\n\nErrors\n\nRaises an ArgumentError if level is not present in bundle (see get_level for details)\n\nExample\n\nvocab = get_vocabulary(bund, :word)\nprintln(\"Top-10 tokens: \", vocab.id_to_token_strings[1:10])\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.has_level-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.has_level","text":"has_level(bundle, level) -> Bool\n\nReturn true if the given PreprocessBundle contains a LevelBundle for the segmentation level level (e.g. :byte, :word, :sentence); otherwise return false.\n\nArguments\n\nbundle::PreprocessBundle â€” bundle to inspect.\nlevel::Symbol            â€” level identifier to look for.\n\nExample\n\njulia> has_level(bund, :word)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.preprocess_corpus-Tuple{Any, PreprocessConfiguration}","page":"API Reference","title":"KeemenaPreprocessing.preprocess_corpus","text":"preprocess_corpus(sources, cfg; save_to = nothing) - PreprocessBundle\n\nVariant of preprocess_corpus that accepts an already constructed PreprocessConfiguration and therefore bypasses all keyword aliasing and default-override logic.\n\nUse this when you have prepared a configuration object up-front (e.g. loaded from disk, shared across jobs, or customised in a function) and want to run the pipeline with those exact settings.\n\nArguments\n\nname type description\nsources AbstractString, Vector{<:AbstractString}, iterable One or more file paths, URLs, directories (ignored), or in-memory text strings.\ncfg PreprocessConfiguration Fully-specified configuration controlling every cleaning/tokenisation option.\nsave_to String or nothing (default) If non-nothing, the resulting bundle is serialised (e.g. via JLD2) to the given file path and returned; otherwise nothing is written.\n\nPipeline (unchanged)\n\nLoad raw sources.\nClean text based on cfg flags.\nTokenise & segment; record requested offsets.\nBuild vocabulary obeying minimum_token_frequency, special_tokens, ...\nPack everything into a PreprocessBundle.  Optionally persist.\n\nReturns\n\nA PreprocessBundle populated with corpora, vocabularies, alignments, metadata, and (by default) empty extras.\n\nExample\n\ncfg  = PreprocessConfiguration(strip_markdown = true,\n                               tokenizer_name  = :unicode)\n\nbund = preprocess_corpus([\"doc1.txt\", \"doc2.txt\"], cfg;\n                         save_to = \"unicode_bundle.jld2\")\n\nnote: If you do not have a configuration object yet, call the keyword-only version instead:       preprocess_corpus(sources; kwargs...)      which will create a default configuration and apply any overrides you provide.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.preprocess_corpus-Tuple{Any}","page":"API Reference","title":"KeemenaPreprocessing.preprocess_corpus","text":"preprocess_corpus(sources; save_to = nothing,\n                              config = nothing,\n                              kwargs...) -> PreprocessBundle\n\nEnd-to-end convenience wrapper that loads raw texts, cleans them, tokenises, builds a vocabulary, records offsets, and packs the result into a PreprocessBundle.\n\nThe routine can be invoked in two mutually-exclusive ways:\n\nExplicit configuration - supply your own PreprocessConfiguration through the config= keyword.\nAd-hoc keyword overrides - omit config and pass any subset of the configuration keywords directly (e.g. lowercase = false, tokenizer_name = :unicode).   Internally a fresh PreprocessConfiguration(; kwargs...) is created from those overrides plus the documented defaults, so calling preprocess_corpus(sources) with no keywords at all runs the pipeline using the default settings.\n\nnote: Passing both config= and per-field keywords is an error because     it would lead to ambiguous intent.\n\nArguments\n\nname type description\nsources AbstractString, Vector{<:AbstractString}, or iterable Either one or more file paths/URLs that will be read, directories (silently skipped), or in-memory strings treated as raw text.\nsave_to String or nothing (default) If a path is given the resulting bundle is serialised (JLD2) to disk and returned; otherwise nothing is written.\nconfig PreprocessConfiguration or nothing Pre-constructed configuration object.  When nothing (default), a new one is built from kwargs....\nkwargs... see PreprocessConfiguration Per-field overrides that populate a fresh configuration when config is nothing.\n\nPipeline stages\n\nLoading - files/URLs are fetched; directory entries are ignored.\nCleaning - controlled by the configuration's cleaning toggles.\nTokenisation & segmentation - produces token ids and offset tables.\nVocabulary building - applies minimum_token_frequency and inserts special tokens.\nPackaging - returns a PreprocessBundle; if save_to was given, the same bundle is persisted to that path.\n\nReturns\n\nA fully-populated PreprocessBundle.\n\nExamples\n\n# 1. Quick start with defaults\nbund = preprocess_corpus(\"corpus.txt\")\n\n# 2. Fine-grained control via keyword overrides\nbund = preprocess_corpus([\"doc1.txt\", \"doc2.txt\"];\n                         strip_html_tags = true,\n                         tokenizer_name  = :unicode,\n                         minimum_token_frequency = 3)\n\n# 3. Supply a hand-crafted configuration object\ncfg  = PreprocessConfiguration(strip_markdown = true,\n                               record_sentence_offsets = false)\nbund = preprocess_corpus(\"input/\", config = cfg, save_to = \"bundle.jld2\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.preprocess_corpus_streaming-Tuple{Any}","page":"API Reference","title":"KeemenaPreprocessing.preprocess_corpus_streaming","text":"preprocess_corpus_streaming(srcs;\n                            cfg           = PreprocessConfiguration(),\n                            vocab         = nothing,\n                            chunk_tokens  = DEFAULT_CHUNK_TOKENS) -> Channel{PreprocessBundle}\n\nLow-memory, two-pass variant of preprocess_corpus that yields a stream of PreprocessBundles via a Channel.   Each bundle covers â‰ˆ chunk_tokens worth of tokens, letting you pipeline huge corpora through training code without ever loading the whole dataset into RAM.\n\nWorkflow\n\nVocabulary pass (optional)   If vocab === nothing, the function first computes global token-frequency counts in a constant-memory scan (_streaming_counts) and builds a vocabulary with build_vocabulary(freqs; cfg).   If you already possess a fixed vocabulary (e.g. for fine-tuning), supply it through the vocab keyword to skip this pass.\nChunking iterator   A background task produced by doc_chunk_iterator groups raw source documents into slices whose estimated size does not exceed chunk_tokens.\nPer-chunk pipeline   For every chunk the following steps mirror the standard pipeline:\nclean_documents\ntokenize_and_segment\nassemble_bundle\nbuild_ensure_alignments!\nThe resulting bundle is put! onto the channel.\n\nArguments\n\nname type description\nsrcs iterable of AbstractString File paths, URLs, or raw texts.\ncfg PreprocessConfiguration Cleaning/tokenisation settings (default: fresh object).\nvocab Vocabulary or nothing Pre-existing vocabulary; when nothing it is inferred in pass 1.\nchunk_tokens Int Soft cap on tokens per chunk (default = DEFAULT_CHUNK_TOKENS).\n\nReturns\n\nA channel of type Channel{PreprocessBundle}.   Consume it with foreach, for bundle in ch, or take!(ch).\n\nch = preprocess_corpus_streaming(\"large_corpus/*\";\n                                 cfg = PreprocessConfiguration(strip_html_tags=true),\n                                 chunk_tokens = 250_000)\n\nfor bund in ch                      # streaming training loop\n    update_model!(bund)             # user-defined function\nend\n\nnote: The channel is unbuffered (Inf capacity) so each bundle is produced only     when the consumer is ready, minimising peak memory consumption.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.with_extras-Tuple{PreprocessBundle, Any}","page":"API Reference","title":"KeemenaPreprocessing.with_extras","text":"with_extras(original, new_extras) -> PreprocessBundle\n\nCreate a shallow copy of original where only the extras field is replaced by new_extras.  All other components (levels, metadata, alignments) are cloned by reference, so the operation is cheap and the returned bundle remains consistent with the source.\n\nUseful when you have performed post-processing (e.g. dimensionality reduction, cluster assignments, per-document labels) and want to attach the results without mutating the original bundle in place.\n\nArguments\n\nname type description\noriginal PreprocessBundle Bundle produced by preprocess_corpus.\nnew_extras Any Arbitrary payload to store under bundle.extras.\n\nReturns\n\nA new PreprocessBundle{typeof(new_extras)} identical to original except that extras == new_extras.\n\nExample\n\nlabels = collect(kmeans(doc_embeddings, 50).assignments)\nlabeled = with_extras(bund, labels)\n\n@assert labeled.levels === bund.levels         # same reference\n@assert labeled.extras === labels              # updated payload\n\n\n\n\n\n","category":"method"},{"location":"api/#Power-user-helpers","page":"API Reference","title":"Power-user helpers","text":"","category":"section"},{"location":"#KeemenaPreprocessing.jl-:smile:","page":"Home","title":"KeemenaPreprocessing.jl   :smile:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Clean â†’ Tokenise/Segment â†’ Bundle","category":"page"},{"location":"","page":"Home","title":"Home","text":"using KeemenaPreprocessing\ncfg    = PreprocessConfiguration()\nbundle = preprocess_corpus(\"data/alice.txt\"; config = cfg)","category":"page"},{"location":"","page":"Home","title":"Home","text":"ðŸ‘‰ See the Guides for worked examples  \nðŸ‘‰ Full API in the reference","category":"page"},{"location":"guides/quickstart/#Quick-Start","page":"Quick Start","title":"Quick Start","text":"","category":"section"},{"location":"guides/quickstart/","page":"Quick Start","title":"Quick Start","text":"using KeemenaPreprocessing\ncfg = PreprocessConfiguration(\n        language  = \"en\",\n        min_freq  = 3,\n        tokenizer_name = :whitespace)\n\nbundle = preprocess_corpus(\"my_corpus.txt\"; config = cfg)\n@show bundle.vocab_size","category":"page"},{"location":"guides/quickstart/","page":"Quick Start","title":"Quick Start","text":"See Configuration for all keyword options.","category":"page"}]
}
