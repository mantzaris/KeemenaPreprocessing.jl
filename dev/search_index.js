var documenterSearchIndex = {"docs":
[{"location":"guides/levels/#Segmentation-Levels-inside-a-PreprocessBundle","page":"Levels","title":"Segmentation Levels inside a PreprocessBundle","text":"A PreprocessBundle stores the same corpus at multiple segmentation levels so that any downstream component can choose the most convenient granularity:\n\nbyte  ->  character  ->  word  ->  sentence  ->  paragraph  ->  document\n\nEach level is wrapped in a LevelBundle; all bundles live in the dictionary bundle.levels.\n\nstruct PreprocessBundle{ExtraT}\n    levels     :: Dict{Symbol,LevelBundle}             # the stack\n    metadata   :: PipelineMetadata                     # conf + schema\n    alignments :: Dict{Tuple{Symbol,Symbol},CrossMap}  # level-to-level\n    extras     :: ExtraT                               # user payload\nend\n\n","category":"section"},{"location":"guides/levels/#Level-cheat-sheet","page":"Levels","title":"Level cheat-sheet","text":"Level symbol Token example Offset vector inside Corpus\n:byte one UTF-8 byte byte_offsets\n:character Unicode scalar value character_offsets\n:word whitespace/unicode-split word word_offsets\n:sentence heuristic sentence span sentence_offsets\n:paragraph blank-line span paragraph_offsets\n:document whole source file document_offsets\n\nUse the lookup table LEVEL_TO_OFFSETS_FIELD to get the field name programmatically:\n\nfield = LEVEL_TO_OFFSETS_FIELD[:sentence]   # :sentence_offsets\nspans = getfield(corpus, field)\n\n","category":"section"},{"location":"guides/levels/#Creating-or-guaranteeing-levels","page":"Levels","title":"Creating or guaranteeing levels","text":"","category":"section"},{"location":"guides/levels/#Built-in-pipelines","page":"Levels","title":"Built-in pipelines","text":"Pipeline call Levels you always get\npreprocess_corpus :byte, :character, :word (+ optional sentence/paragraph)\npreprocess_corpus_streaming* same per chunk; merge helper rebuilds alignments","category":"section"},{"location":"guides/levels/#Helper-functions","page":"Levels","title":"Helper functions","text":"_ensure_lower_levels!(bundle)      # synthesise :character / :byte if missing\nbuild_alignments!(bundle)          # create cross-maps already available levels\nbuild_ensure_alignments!(bundle)   # do both, idempotent\n\n","category":"section"},{"location":"guides/levels/#Quick-examples","page":"Levels","title":"Quick examples","text":"","category":"section"},{"location":"guides/levels/#Inspect-the-word-vocabulary","page":"Levels","title":"Inspect the word vocabulary","text":"bund   = preprocess_corpus(\"books/*\")\nwvocab = bund.levels[:word].vocabulary\nprintln(\"vocab size = \", length(wvocab.id_to_token_strings))","category":"section"},{"location":"guides/levels/#Map-byte-123-to-its-word-index","page":"Levels","title":"Map byte 123 to its word index","text":"build_ensure_alignments!(bund)            # guarantee maps exist\nword_idx = bund.alignments[(:byte,:word)].alignment[123]","category":"section"},{"location":"guides/levels/#Add-a-custom-feature-matrix-as-extras","page":"Levels","title":"Add a custom feature matrix as extras","text":"feats = rand(Float32, length(get_token_ids(bund,:word)), 128)\nbund  = PreprocessBundle(bund.levels;\n                         metadata   = bund.metadata,\n                         alignments = bund.alignments,\n                         extras     = feats)\n\n","category":"section"},{"location":"guides/levels/#Offset-vector-convention","page":"Levels","title":"Offset-vector convention","text":"Every offset vector satisfies\n\nissorted(vec) == true\nvec[1]        ∈ (0, 1)        # leading sentinel\nvec[end]      ≥ n_tokens      # trailing sentinel (n  or  n+1)\n\nThe streaming merge helper recognises both sentinel styles.\n\n","category":"section"},{"location":"guides/levels/#Common-pitfalls","page":"Levels","title":"Common pitfalls","text":"Pitfall Remedy\nMissing :byte / :character levels after custom manipulation Call build_ensure_alignments!(bundle) once more.\nAccessing an alignment that isn’t there → KeyError Check keys(bundle.alignments) or call build_alignments!.\nOffset validation failure when you inject your own Corpus Run validate_offsets(corpus, :level) before constructing LevelBundle.\n\n","category":"section"},{"location":"guides/levels/#APIs-at-a-glance","page":"Levels","title":"APIs at a glance","text":"_ensure_lower_levels!(bundle::PreprocessBundle)          -> PreprocessBundle\nbuild_alignments!(bundle; pairs=[...])                   -> PreprocessBundle\nbuild_ensure_alignments!(bundle)                         -> PreprocessBundle\nLEVEL_TO_OFFSETS_FIELD::Dict{Symbol,Symbol}\n\nWith these utilities your bundles are always multi-level, aligned, and self-describing.","category":"section"},{"location":"guides/types/#Core-Types-Introduction","page":"Core Types Introduction","title":"Core Types Introduction","text":"This page gives a practical introduction to the small set of types that underline KeemenaPreprocessing.jl. It is aimed at users who want to understand the data model well enough to slice, align, and feed text into downstream models without reading the entire API reference.\n\n","category":"section"},{"location":"guides/types/#Basic-Concept","page":"Core Types Introduction","title":"Basic Concept","text":"Keemena represents a corpus as a flat vector of token identifiers plus offset vectors that mark the starts of higher‑level units:\n\nbytes -> characters -> words -> sentences -> paragraphs -> documents\n\nEach segmentation level is packaged as a LevelBundle (a Corpus + its Vocabulary). A PreprocessBundle is the top‑level structure that stores all levels, cross‑level alignment tables, a reproducibility header, and optional user payload ('extras').\n\n","category":"section"},{"location":"guides/types/#PreprocessBundle{ExtraT}","page":"Core Types Introduction","title":"PreprocessBundle{ExtraT}","text":"Top‑level structure returned by preprocess_corpus (and the streaming variants).\n\nFields\n\nlevels :: Dict{Symbol,LevelBundle} - the available segmentation levels (e.g. :byte, :character, :word, :sentence, :paragraph, :document).\nmetadata :: PipelineMetadata - reproducibility header (configuration + on‑disk schema version).\nalignments :: Dict{Tuple{Symbol,Symbol},CrossMap} - cross‑level projections keyed as (source, destination), eg. (:byte, :word)\nextras :: ExtraT - optional user payload (feature matrices, raw text, tags, etc.).\n\nCommon operations\n\nusing KeemenaPreprocessing\n\n# Load or create a bundle\nbundle = preprocess_corpus(\"corpus/*.txt\")\n\n# Inspect available levels\navailable_levels = collect(keys(bundle.levels))  # eg., [:byte, :character, :word]\n\n# Access a level's vocabulary\nword_vocabulary = bundle.levels[:word].vocabulary\nprintln(\"vocabulary size: \", length(word_vocabulary.id_to_token_strings))\n\n# Ensure canonical cross-level maps (:byte -> :character -> :word)\nbuild_ensure_alignments!(bundle)\n\n# Membership query: which word contains byte index i?\n# (requires the canonical alignments, ensured above)\nbyte_index = 1\nword_index_of_byte1 = bundle.alignments[(:byte, :word)].alignment[byte_index]\n\n","category":"section"},{"location":"guides/types/#LevelBundle","page":"Core Types Introduction","title":"LevelBundle","text":"Pairing of a Corpus with its companion Vocabulary for one segmentation level:\n\nFields\n\ncorpus     :: Corpus\nvocabulary :: Vocabulary\n\nIntegrity checks ensure all token identifiers in corpus.token_ids are valid for the vocabulary:\n\nExample\n\nword_level  = bundle.levels[:word]\nword_corpus = word_level.corpus\nword_vocab  = word_level.vocabulary\n\n","category":"section"},{"location":"guides/types/#Corpus","page":"Core Types Introduction","title":"Corpus","text":"A compact container for all token identifiers at a level and optional offset tables that recover structure:\n\nFields\n\ntoken_ids          :: Vector{Int} (always present)\ndocument_offsets   :: Vector{Int} (always present)\nparagraph_offsets  :: Union{Vector{Int},Nothing}\nsentence_offsets   :: Union{Vector{Int},Nothing}\nword_offsets       :: Union{Vector{Int},Nothing}\ncharacter_offsets  :: Union{Vector{Int},Nothing}\nbyte_offsets       :: Union{Vector{Int},Nothing}\n\nEach offset vector marks start positions (1‑based, inclusive). When present, it is sentinel‑terminated so that view(token_ids, offsets[i] : offsets[i+1]-1) yields the i‑th unit.","category":"section"},{"location":"guides/types/#Offset-invariants-(what-'valid'-means)","page":"Core Types Introduction","title":"Offset invariants (what 'valid' means)","text":"Offset vectors are monotone, begin with a leading sentinel 0 or 1, and end with n_tokens or n_tokens+1 depending on level. The streaming merge helper normalizes both styles. See Guides -> Offsets for the per‑level conventions and the LEVEL_TO_OFFSETS_FIELD lookup.","category":"section"},{"location":"guides/types/#Practical-slices","page":"Core Types Introduction","title":"Practical slices","text":"# Word-level corpus\nword_corpus = bundle.levels[:word].corpus\n\n# First document as a view of token_ids\ndoc1_rng    = word_corpus.document_offsets[1] : word_corpus.document_offsets[2]-1\ndoc1_ids    = view(word_corpus.token_ids, doc1_rng)\n\n# First sentence tokens (if recorded)\nif word_corpus.sentence_offsets !== nothing\n    s1 = view(word_corpus.token_ids,\n              word_corpus.sentence_offsets[1] : word_corpus.sentence_offsets[2]-1)\nend\n\n","category":"section"},{"location":"guides/types/#Vocabulary","page":"Core Types Introduction","title":"Vocabulary","text":"An immutable bidirectional mapping between strings and integer token identifiers, plus frequency information and special tokens.\n\nFields\n\nid_to_token_strings :: Vector{String}\ntoken_to_id_map     :: Dict{String,Int}\ntoken_frequencies   :: Vector{Int}\nspecial_tokens      :: Dict{Symbol,Int} (e.g., :unk, :pad, :bos, :eos)\n\nTypical lookups\n\nword_vocabulary = bundle.levels[:word].vocabulary\n\n# id -> string\nfirst_10_strings = word_vocabulary.id_to_token_strings[1:10]\n\n# string -> id (falls back to :unk if absent)\nunknown_id  = word_vocabulary.special_tokens[:unk]\nhello_id    = get(word_vocabulary.token_to_id_map, \"hello\", unknown_id)\n\n","category":"section"},{"location":"guides/types/#CrossMap-(alignment-between-levels)","page":"Core Types Introduction","title":"CrossMap (alignment between levels)","text":"CrossMap is a lightweight record for level‑to‑level lookup. In practice you will encounter membership maps for the canonical chain\n\n(:byte -> :character), (:byte -> :word), (:character -> :word)\n\nthis can answer 'which destination unit contains this source index?' in O(1) lookups.\n\nFields\n\nsource_level      :: Symbol\ndestination_level :: Symbol\nalignment         :: Vector{Int}","category":"section"},{"location":"guides/types/#Membership-maps-(fine-coarse)","page":"Core Types Introduction","title":"Membership maps (fine -> coarse)","text":"The default alignments built by build_ensure_alignments! are membership maps:\n\nbuild_ensure_alignments!(bundle)  # guarantees the canonical trio exists\n\n# Byte 123 -> word index (O(1))\nword_index = bundle.alignments[(:byte, :word)].alignment[123]\n\n# Character 42 -> word index\nchar_to_word = bundle.alignments[(:character, :word)]\nword_ix      = char_to_word.alignment[42]\n\nThese membership maps have length(alignment) == n_source_tokens (no sentinel): each fine‑grained token is labeled with its containing coarse unit.","category":"section"},{"location":"guides/types/#Range-maps-(coarse-fine)","page":"Core Types Introduction","title":"Range maps (coarse -> fine)","text":"For some tasks you want the span of fine tokens that make up a coarse unit. You can derive those spans directly from the coarse level's offsets:\n\n# Word-level spans of the k-th sentence:\nwc          = bundle.levels[:word].corpus\nk           = 1\nsent_word_i = wc.sentence_offsets[k] : wc.sentence_offsets[k+1] - 1\n\n(If you construct a 'range' CrossMap yourself, its alignment uses sentinel style: length(alignment) == n_destination + 1, and alignment[i] : alignment[i+1]-1 is the source span of destination unit i.)\n\n","category":"section"},{"location":"guides/types/#Some-recipes","page":"Core Types Introduction","title":"Some recipes","text":"Words per sentence\n\nwc = bundle.levels[:word].corpus\nsentence_lengths = diff(wc.sentence_offsets)  # requires recorded sentence offsets\n\nPer‑document token views\n\nwc = bundle.levels[:word].corpus\ndoc_offs = wc.document_offsets\ndoc_views = [view(wc.token_ids, doc_offs[i] : doc_offs[i+1]-1)\n             for i in 1:length(doc_offs)-1]\n\nRound‑trip: word -> raw‑text span   (assuming the raw text is stored in bundle.extras.raw_text)\n\nwc      = bundle.levels[:word].corpus\nw_idx   = 42\nspan    = wc.word_offsets[w_idx] : wc.word_offsets[w_idx+1]-1\nraw_str = String(codeunits(bundle.extras.raw_text)[span])\n\n","category":"section"},{"location":"guides/types/#Summary","page":"Core Types Introduction","title":"Summary","text":"get_corpus(bundle, :word) - convenience accessor for level corpora (used throughout the guides).\nget_token_ids(bundle, :word) - direct access to the flattened id sequence for a level.\nbuild_ensure_alignments!(bundle) - guarantees the canonical membership maps exist.\nLEVEL_TO_OFFSETS_FIELD -> symbol -> field lookup for offset vectors (e.g., :sentence -> :sentence_offsets).","category":"section"},{"location":"guides/tokenizers/#Using-KeemenaPreprocessing-with-existing-tokenizers","page":"Using Existing Tokenizers","title":"Using KeemenaPreprocessing with existing tokenizers","text":"KeemenaPreprocessing is not a tokenizer collection and it is not a general NLP toolkit. Instead, it is a preprocessing pipeline: you choose how to tokenize, and KeemenaPreprocessing builds a deterministic PreprocessBundle (tokens + vocabulary + offsets + cross-level alignments) for downstream modeling, annotation, and evaluation.\n\nThis page shows how to use existing tokenizer packages (eg WordTokenizers.jl, BytePairEncoding.jl) without KeemenaPreprocessing taking hard dependencies on them. KeemenaPreprocessing also ships a small set of built-in tokenizers (:whitespace, :unicode, :byte, :char) for convenience with various configurations that should work in most instances.\n\n:word is the canonical key for the primary string-token stream produced by tokenizer_name (whitespace tokens, BPE pieces, grapheme clusters, etc)\n:character is the canonical key for the Unicode scalar-value stream (codepoints) when recorded\n:byte is the canonical key for the UTF-8 byte stream when recorded.\n\n","category":"section"},{"location":"guides/tokenizers/#Why-callables-(avoid-hard-dependencies)","page":"Using Existing Tokenizers","title":"Why callables (avoid hard dependencies)","text":"KeemenaPreprocessing accepts tokenizers as callables rather than integrating tightly with any single tokenization ecosystem:\n\nReproducibility: the tokenizer behavior is pinned inside your PreprocessConfiguration, rather than being controlled by hidden global defaults.\nAvoid global state surprises: some tokenizer packages expose globally configurable defaults (changing them can affect other code in the same Julia session)\nAPI stability: upstream tokenizers evolve; a minimal callable interface keeps KeemenaPreprocessing stable and reduces breakage risk.\nSeparation of concerns: KeemenaPreprocessing focuses on streaming execution, offsets, alignment, and deterministic artifacts, not on maintaining feature parity with every tokenizer.\n\n","category":"section"},{"location":"guides/tokenizers/#Tokenizer-interface-contract","page":"Using Existing Tokenizers","title":"Tokenizer interface contract","text":"PreprocessConfiguration(tokenizer_name = ...) accepts either:\n\na built-in symbol (:whitespace, :unicode, :byte, :char), or\na custom callable tokenizer(::AbstractString) -> Vector{<:AbstractString}.\n\nIn other words:\n\ntokens = tokenizer(text::AbstractString)\n# tokens must be a Vector of strings (String, SubString{String}, etc.)\n\n","category":"section"},{"location":"guides/tokenizers/#Canonical-level-naming","page":"Using Existing Tokenizers","title":"Canonical level naming","text":"KeemenaPreprocessing uses canonical level keys in the returned bundle:\n\n:word is the canonical key for the primary string-token stream (this includes subword pieces such as BPE).\n:character is used for grapheme-level tokenization (tokenizer_name = :char).\n:byte is used for raw UTF-8 bytes (tokenizer_name = :byte).\n\nword_corpus = get_corpus(bundle, :word)\nchar_corpus = get_corpus(bundle, :character)\nbyte_corpus = get_corpus(bundle, :byte)\n\n","category":"section"},{"location":"guides/tokenizers/#WordTokenizers.jl-(concrete-examples)","page":"Using Existing Tokenizers","title":"WordTokenizers.jl (concrete examples)","text":"WordTokenizers.jl exports multiple explicit tokenizers such as nltk_word_tokenize, toktok_tokenize, penn_tokenize, and others.","category":"section"},{"location":"guides/tokenizers/#Note-on-global-configuration","page":"Using Existing Tokenizers","title":"Note on global configuration","text":"WordTokenizers allows changing global defaults via set_tokenizer(...) and set_sentence_splitter(...). Changing these defaults can affect other packages (or other parts of your code) that call WordTokenizers.tokenize or WordTokenizers.split_sentences using the global defaults.\n\nFor reproducible pipelines, prefer calling the tokenizer you want explicitly (as below).","category":"section"},{"location":"guides/tokenizers/#Example:-NLTK-like-word-tokenization","page":"Using Existing Tokenizers","title":"Example: NLTK-like word tokenization","text":"using KeemenaPreprocessing\nimport WordTokenizers\n\n# Wrap WordTokenizers output so we always return Vector{String}\nfunction nltk_word_tokens_as_strings(text::AbstractString)::Vector{String}\n    return String.(WordTokenizers.nltk_word_tokenize(text))\nend\n\ncfg = PreprocessConfiguration(\n    tokenizer_name = nltk_word_tokens_as_strings,\n    record_sentence_offsets = true,   # optional: record sentence boundaries\n    minimum_token_frequency = 1,      # convenient for quick experiments\n)\n\ndocuments = [\n    \"Hello, world! This is a test.\",\n    \"Email me at example@test.com and visit https://example.com.\",\n    \"Don't split 3.14 weirdly; keep punctuation sensible.\"\n]\n\nbundle = preprocess_corpus(documents; config = cfg)\n\nprintln(\"Levels present: \", collect(keys(bundle.levels)))\nword_corpus = get_corpus(bundle, :word)\nprintln(\"Number of tokens: \", length(word_corpus.token_ids))","category":"section"},{"location":"guides/tokenizers/#Example:-Penn-Treebank-tokenizer","page":"Using Existing Tokenizers","title":"Example: Penn Treebank tokenizer","text":"using KeemenaPreprocessing\nimport WordTokenizers\n\nfunction penn_tokens_as_strings(text::AbstractString)::Vector{String}\n    return String.(WordTokenizers.penn_tokenize(text))\nend\n\ncfg = PreprocessConfiguration(\n    tokenizer_name = penn_tokens_as_strings,\n    record_sentence_offsets = true,\n    minimum_token_frequency = 1,\n)\n\ndocuments = [\n    \"Mr. Smith can't attend today; he's busy.\",\n    \"She said: \\\"Tokenize this properly!\\\"\"\n]\n\nbundle = preprocess_corpus(documents; config = cfg)\nprintln(\"Levels present: \", collect(keys(bundle.levels)))\n\n","category":"section"},{"location":"guides/tokenizers/#BytePairEncoding.jl-(conceptual-practical-examples)","page":"Using Existing Tokenizers","title":"BytePairEncoding.jl (conceptual + practical examples)","text":"BytePairEncoding.jl provides BPE tokenizers (including OpenAI-style tiktoken and GPT-2 byte-level BPE)\n\nBytePairEncoding.load_tiktoken(...) returns a callable tokenizer object that yields Vector{String} token pieces\n\nimport BytePairEncoding\n\nbpe = BytePairEncoding.load_tiktoken(\"cl100k_base\")\n\nprintln(\"Is Function? \", bpe isa Function)\nprintln(\"Callable for AbstractString? \", applicable(bpe, \"Hello world!\"))\n\npieces = bpe(\"Hello world! This is BPE.\")\nprintln(pieces)\n\nOutput looks like subword-ish pieces (often including leading-space pieces like \" world\").","category":"section"},{"location":"guides/tokenizers/#Recommended,-wrap-the-callable-object-in-a-plain-function","page":"Using Existing Tokenizers","title":"Recommended, wrap the callable object in a plain function","text":"Some configurations validate tokenizer_name with isa Function. Since BPETokenizer is callable but not a Function, the most robust pattern is to wrap it:\n\nusing KeemenaPreprocessing\nimport BytePairEncoding\n\nbpe = BytePairEncoding.load_tiktoken(\"cl100k_base\")\n\n# Wrap the callable struct in a plain function.\nbpe_function(text::AbstractString)::Vector{String} = bpe(text)\n\ncfg = PreprocessConfiguration(\n    tokenizer_name = bpe_function,\n\n    # Recommended for BPE: avoid splitting text before tokenization.\n    # Many BPE tokenizers are sensitive to boundary context.\n    record_sentence_offsets = false,\n    record_paragraph_offsets = false,\n\n    minimum_token_frequency = 1,\n)\n\ndocuments = [\n    \"Hello world! This is a test of BPE tokenization.\",\n    \"Email me at example@test.com and visit https://example.com.\"\n]\n\nbundle = preprocess_corpus(documents; config = cfg)\n\nprintln(\"Levels present: \", collect(keys(bundle.levels)))\n\n# The primary string-token segmentation is retrieved as :word\nsubword_corpus = get_corpus(bundle, :word)\nprintln(\"Number of BPE tokens: \", length(subword_corpus.token_ids))\n\nsubword_vocab = get_vocabulary(bundle, :word)\nprintln(\"First 30 vocab strings: \", subword_vocab.id_to_token_strings[1:min(end, 30)])\n\nImportant: Keemena stores the primary string-token stream under :word even when the tokenizer returns subword pieces (BPE).","category":"section"},{"location":"guides/tokenizers/#byte-and-character-offsets-with-BPE","page":"Using Existing Tokenizers","title":"byte and character offsets with BPE","text":"KeemenaPreprocessing supports byte-level and character-level tokenization via built-in tokenizers:\n\ntokenizer_name = :byte for raw UTF-8 bytes (level :byte)\ntokenizer_name = :char for Unicode graphemes (level :character)\n\nIf you request record_byte_offsets=true or record_character_offsets=true, make sure your configuration and your KeemenaPreprocessing version support doing so with your chosen tokenizer. If you are using a BPE tokenizer and you need explicit byte/character token streams, the simplest robust approach is often to run an additional preprocessing pass using :byte or :char (with the same cleaning settings) and keep that with your BPE-based :word stream.\n\n","category":"section"},{"location":"guides/tokenizers/#Getting-token-IDs-from-BytePairEncoding-(encoder)","page":"Using Existing Tokenizers","title":"Getting token IDs from BytePairEncoding (encoder)","text":"If you need the exact integer IDs used by a tiktoken-style encoder, BytePairEncoding can load an encoder separately:\n\nimport BytePairEncoding\n\nenc = BytePairEncoding.load_tiktoken_encoder(\"cl100k_base\")\nids = enc.encode(\"hello world\")   # Vector{Int}\nprintln(ids)","category":"section"},{"location":"api/#Public-API","page":"API Reference","title":"Public API","text":"","category":"section"},{"location":"api/#KeemenaPreprocessing.TOKENIZERS","page":"API Reference","title":"KeemenaPreprocessing.TOKENIZERS","text":"TOKENIZERS\n\nA constant Tuple{Symbol} listing the names of built-in tokenizers that can be passed to the tokenizer_name keyword of PreprocessConfiguration.\n\nCurrently supported values are\n\n:whitespace - split on Unicode whitespace;\n:unicode    - iterate user-perceived graphemes (eachgrapheme);\n:byte       - treat the text as raw bytes (byte-level models);\n:char       - split on individual UTF-8 code units.\n\nYou may also supply any callable that implements mytokens = f(string) in place of one of these symbols.\n\n\n\n\n\n","category":"constant"},{"location":"api/#KeemenaPreprocessing.Corpus","page":"API Reference","title":"KeemenaPreprocessing.Corpus","text":"Corpus\n\nFlat, memory-efficient container that stores an entire corpus of token-ids together with optional hierarchical offset tables that recover the original structure (documents → paragraphs → sentences → words → characters → bytes).\n\nEvery offset vector records the starting index (1-based, inclusive) of each unit inside token_ids.  The final entry therefore equals length(token_ids)+1, making range retrieval convenient via view(token_ids, offsets[i] : offsets[i+1]-1).\n\nFields\n\nfield type always present? description\ntoken_ids Vector{Int} ✓ Concatenated token identifiers returned by the vocabulary.\ndocument_offsets Vector{Int} ✓ Start positions of each document (outermost level).\nparagraph_offsets Union{Vector{Int},Nothing} cfg-dependent Paragraph starts within each document when record_paragraph_offsets=true.\nsentence_offsets Union{Vector{Int},Nothing} cfg-dependent Sentence boundaries when record_sentence_offsets=true.\nword_offsets Union{Vector{Int},Nothing} cfg-dependent Word boundaries when record_word_offsets=true.\ncharacter_offsets Union{Vector{Int},Nothing} cfg-dependent Unicode-character spans when record_character_offsets=true.\nbyte_offsets Union{Vector{Int},Nothing} cfg-dependent Byte-level spans when record_byte_offsets=true.\n\nExample\n\n# assume `corp` is a Corpus produced by preprocess_corpus\ndoc1_range = corp.document_offsets[1] : corp.document_offsets[2]-1\ndoc1_token_ids = view(corp.token_ids, doc1_range)\n\nif corp.sentence_offsets ≠ nothing\n    first_sentence = view(corp.token_ids,\n                          corp.sentence_offsets[1] : corp.sentence_offsets[2]-1)\nend\n\nThe presence or absence of each optional offsets vector is determined entirely by the corresponding record_*_offsets flags in PreprocessConfiguration.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.CrossMap","page":"API Reference","title":"KeemenaPreprocessing.CrossMap","text":"CrossMap\n\nAlignment table that links two segmentation levels of the same corpus (e.g. bytes -> characters, characters -> words, words -> sentences).\n\nFor every unit in the destination level the alignment vector stores the 1-based index into the source offsets at which that unit begins. This allows constant-time projection of any span expressed in destination units back to the finer-grained source sequence.\n\nFields\n\nsource_level      :: Symbol   Name of the finer level (must match a key in bundle.levels, typically :byte, :char, :word, :sentence, or :paragraph).\ndestination_level :: Symbol   Name of the coarser level whose boundaries are encoded.\nalignment         :: Vector{Int}   Length = N_destination + 1.   alignment[i] is the starting source-level offset of destination element i; the extra sentinel entry alignment[end] = N_source + 1 lets you slice with   alignment[i] : alignment[i+1]-1 without bounds checks.\n\nExample\n\n# map words ⇒ sentences\nm = CrossMap(:word, :sentence, sent2word_offsets)\n\nfirst_sentence_word_ids = alignment_view(m, 1)  # helper returning a view\n\nThe constructor is trivial and performs no validation; pipelines are expected to guarantee consistency when emitting CrossMap objects.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.CrossMap-Tuple{Symbol, Symbol, AbstractVector{<:Integer}}","page":"API Reference","title":"KeemenaPreprocessing.CrossMap","text":"CrossMap(src, dst, align)\n\nShorthand outer constructor that builds a CrossMap while materialising the alignment vector as Vector{Int}.\n\nArguments\n\nsrc::Symbol - identifier of the source (finer-grained) level   (e.g. :char, :word).\ndst::Symbol - identifier of the destination (coarser) level   (e.g. :word, :sentence).\nalign::AbstractVector{<:Integer} - offset array mapping every destination unit to its starting position in the source sequence.  Any integer-typed vector is accepted; it is copied into a dense Vector{Int} to guarantee contiguous storage and type stability inside the resulting CrossMap.\n\nReturns\n\nA CrossMap(src, dst, Vector{Int}(align)).\n\nExample\n\ncm = CrossMap(:char, :word, UInt32[1, 5, 9, 14])\n@assert cm.alignment isa Vector{Int}\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.LevelBundle","page":"API Reference","title":"KeemenaPreprocessing.LevelBundle","text":"LevelBundle\n\nSelf-contained pairing of a Corpus and its companion Vocabulary.  A LevelBundle represents one segmentation level (e.g. words, characters, or bytes) produced by the preprocessing pipeline. By storing both objects side-by-side it guarantees that every token_id found in corpus.token_ids is valid according to vocabulary.\n\nFields\n\ncorpus     :: Corpus   All token-ids plus optional offset tables describing the structure of the text at this level.\nvocabulary :: Vocabulary   Bidirectional mapping between token strings and the integer ids used in corpus.token_ids.\n\nIntegrity checks\n\nThe inner constructor performs two runtime validations:\n\nRange check - the largest token-id must not exceed length(vocabulary.id_to_token_strings).\nLower bound - all token-ids must be >= 1 (id 0 is never legal).\n\nViolations raise an informative ArgumentError, catching mismatches early.\n\nExample\n\nword_corpus  = Corpus(word_ids, doc_offs, nothing, sent_offs, word_offs,\n                      nothing, nothing)\nword_vocab   = build_vocabulary(words; minimum_token_frequency = 2)\n\nword_bundle  = LevelBundle(word_corpus, word_vocab)\n\nnb_tokens    = length(word_bundle.vocabulary.id_to_token_strings)\n@info \"bundle contains nb_tokens unique tokens\"\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.PipelineMetadata","page":"API Reference","title":"KeemenaPreprocessing.PipelineMetadata","text":"PipelineMetadata\n\nCompact header bundled with every artefact produced by KeemenaPreprocessing. It records the exact pipeline settings and the version of the on-disk schema so that data can be re-processed, inspected, or migrated safely.\n\nFields\n\nconfiguration::PreprocessConfiguration   The full set of cleaning, tokenisation, vocabulary, and offset-recording options that generated the artefact.  Storing this ensures strict reproducibility.\nschema_version::VersionNumber   The version of the bundle file format (not the Julia package).   Increment the major component when breaking changes are introduced so that loaders can detect incompatibilities and perform migrations or raise errors.\n\nExample\n\ncfg  = PreprocessConfiguration(strip_html_tags = true)\nmeta = PipelineMetadata(cfg, v\"1.0.0\")\n\n@info \"tokeniser:\" meta.configuration.tokenizer_name\n@assert meta.schema_version >= v\"1.0.0\"\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.PipelineMetadata-Tuple{}","page":"API Reference","title":"KeemenaPreprocessing.PipelineMetadata","text":"PipelineMetadata() -> PipelineMetadata\n\nConvenience constructor that returns a metadata header with\n\nthe default PreprocessConfiguration() (all keyword-arguments left at their documented defaults); and\nthe current bundle schema version v\"1.0.0\".\n\nHandy for rapid prototyping or unit tests when you do not need to customise the pipeline but still require a valid PipelineMetadata object.\n\nIdentical to:\n\nPipelineMetadata(PreprocessConfiguration(), v\"1.0.0\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.PreprocessBundle","page":"API Reference","title":"KeemenaPreprocessing.PreprocessBundle","text":"PreprocessBundle{ExtraT}\n\nTop-level artefact emitted by preprocess_corpus (or the streaming variant). A bundle contains everything required to feed a downstream model or to reload a corpus without re-running the expensive preprocessing pipeline.\n\nType parameter\n\nExtraT - arbitrary payload for user-defined information (e.g. feature matrices, clustering assignments, language tags).  Use Nothing when no extras are needed.\n\nFields\n\nfield type description\nlevels Dict{Symbol,LevelBundle} Mapping from segmentation level name (:byte, :char, :word, :sentence, :paragraph, …) to the corresponding LevelBundle.\nmetadata PipelineMetadata Reproducibility header (configuration + schema version).\nalignments Dict{Tuple{Symbol,Symbol},CrossMap} Pair-wise offset projections between levels, keyed as (source, destination) (e.g. (:char, :word)).\nextras ExtraT Optional user payload carried alongside the core data.\n\nTypical workflow\n\nbund = preprocess_corpus(files; strip_html_tags=true)\n\n# inspect vocabulary\nword_vocab = bund.levels[:word].vocabulary\nprintln(\"vocabulary size: \", length(word_vocab.id_to_token_strings))\n\n# project a sentence span back to character offsets\ncm = bund.alignments[(:char, :sentence)]\nfirst_sentence_char_span = cm.alignment[1] : cm.alignment[2]-1\n\nThe bundle is immutable; to add additional levels or extras create a fresh instance (helper functions add_level!, with_extras, etc. are provided by the package).\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing.PreprocessBundle-Tuple{Dict{Symbol, <:LevelBundle}}","page":"API Reference","title":"KeemenaPreprocessing.PreprocessBundle","text":"PreprocessBundle(levels; metadata = PipelineMetadata(),\n                      alignments = Dict{Tuple{Symbol,Symbol},CrossMap}(),\n                      extras = nothing) -> PreprocessBundle\n\nOuter constructor that validates and assembles the individual artefacts generated by KeemenaPreprocessing into a single PreprocessBundle.\n\nRequired argument\n\nlevels::Dict{Symbol,<:LevelBundle} - at least one segmentation level (keyed by level name such as :word or :char).\n\nOptional keyword arguments\n\nkeyword default purpose\nmetadata PipelineMetadata() Configuration & schema header.\nalignments empty Dict Maps (source,destination) -> CrossMap.\nextras nothing User-supplied payload propagated unchanged.\n\nRuntime checks\n\nNon-empty levels.\nFor each (lvl, lb) in levels run validate_offsets(lb.corpus, lvl) to ensure internal offset consistency.\nFor every supplied alignment (src,dst) → cm:\nboth src and dst must exist in levels;\nlength(cm.alignment) == length(levels[src].corpus.token_ids);\ncm.source_level      == src;\ncm.destination_level == dst.\n\nAny violation throws an informative ArgumentError.\n\nReturns\n\nA fully-validated PreprocessBundle{typeof(extras)} containing: Dict(levels), metadata, Dict(alignments), and extras.\n\nExample\n\nword_bundle = LevelBundle(word_corpus, word_vocab)\nchar_bundle = LevelBundle(char_corpus, char_vocab)\n\nbund = PreprocessBundle(Dict(:word=>word_bundle, :char=>char_bundle);\n                        alignments = Dict((:char,:word)=>char2word_map))\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.PreprocessBundle-Tuple{}","page":"API Reference","title":"KeemenaPreprocessing.PreprocessBundle","text":"PreprocessBundle(; metadata = PipelineMetadata(), extras = nothing) -> PreprocessBundle\n\nConvenience constructor that produces an empty PreprocessBundle:\n\nlevels     = Dict{Symbol,LevelBundle}()  \nalignments = Dict{Tuple{Symbol,Symbol},CrossMap}()  \nmetadata   = metadata (defaults to PipelineMetadata())  \nextras     = extras   (defaults to nothing)\n\nUseful when you want to build a bundle incrementally—for example, loading individual levels from disk or generating them in separate jobs: while still attaching a common metadata header or arbitrary user payload.\n\nbund = PreprocessBundle()                      # blank skeleton\nbund = merge(bund, load_word_level(\"word.jld\"))  # pseudo-code for adding data\n\nThe returned object's type parameter is inferred from extras so that any payload, including complex structs, can be stored without further boilerplate.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.PreprocessConfiguration-Tuple{}","page":"API Reference","title":"KeemenaPreprocessing.PreprocessConfiguration","text":"PreprocessConfiguration(; kwargs...) -> PreprocessConfiguration\n\nCreate a fully-specified preprocessing configuration.\n\nAll keyword arguments are optional; sensible defaults are provided so that cfg = PreprocessConfiguration() already yields a working pipeline.  Options are grouped below by the stage they affect.\n\nCleaning stage toggles\n\nkeyword default purpose\nlowercase true Convert letters to lower-case.\nstrip_accents true Remove combining accent marks.\nremove_control_characters true Drop Unicode Cc/Cf code-points.\nremove_punctuation true Strip punctuation & symbol characters.\nnormalise_whitespace true Collapse consecutive whitespace.\nremove_zero_width_chars true Remove zero-width joiners, etc.\npreserve_newlines true Keep explicit line breaks.\ncollapse_spaces true Collapse runs of spaces/tabs.\ntrim_edges true Strip leading/trailing whitespace.\n\nURL, e-mail & numbers\n\nkeyword default purpose\nreplace_urls true Replace URLs with url_sentinel.\nreplace_emails true Replace e-mails with mail_sentinel.\nkeep_url_scheme false Preserve http:// / https:// prefix.\nurl_sentinel \"<URL>\" Token inserted for each URL.\nmail_sentinel \"<EMAIL>\" Token inserted for each e-mail.\nreplace_numbers false Replace numbers with number_sentinel.\nnumber_sentinel \"<NUM>\" Token used when replacing numbers.\nkeep_number_decimal false Preserve decimal part.\nkeep_number_sign false Preserve ± sign.\nkeep_number_commas false Preserve thousands separators.\n\nMark-up & HTML\n\nkeyword default purpose\nstrip_markdown false Remove Markdown formatting.\npreserve_md_code true Keep fenced/inline code while stripping.\nstrip_html_tags false Remove HTML/XML tags.\nhtml_entity_decode true Decode &amp;, &quot;, etc.\n\nEmoji & Unicode\n\nkeyword default purpose\nemoji_handling :keep :keep, :remove, or :sentinel.\nemoji_sentinel \"<EMOJI>\" Used when emoji_handling == :sentinel.\nsqueeze_repeat_chars false Limit repeated character runs.\nmax_char_run 3 Maximum run length when squeezing.\nmap_confusables false Map visually-confusable chars.\nunicode_normalisation_form :none :NFC, :NFD, :NFKC, :NFKD, or :none.\nmap_unicode_punctuation false Replace Unicode punctuation with ASCII.\n\nTokenisation\n\nkeyword default purpose\ntokenizer_name :whitespace One of TOKENIZERS or a callable.\npreserve_empty_tokens false Keep zero-length tokens.\n\nVocabulary construction\n\nkeyword default purpose\nminimum_token_frequency 1 Discard rarer tokens / map to <UNK>.\nspecial_tokens Dict(:unk=>\"<UNK>\", :pad=>\"<PAD>\") Role ⇒ literal mapping.\n\nOffset recording\n\nkeyword default purpose\nrecord_byte_offsets false Record byte-level spans.\nrecord_character_offsets false Record Unicode-char offsets.\nrecord_word_offsets true Record word offsets.\nrecord_sentence_offsets true Record sentence offsets.\nrecord_paragraph_offsets false Record paragraph offsets (forces preserve_newlines = true).\nrecord_document_offsets true Record document offsets.\n\nReturns\n\nA fully-initialised PreprocessConfiguration instance.  Invalid combinations raise AssertionError (e.g. unsupported tokenizer) and certain settings emit warnings when they imply other flags (e.g. paragraph offsets -> preserve_newlines).\n\nSee also: TOKENIZERS and byte_cfg for a pre-canned byte-level configuration.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.Vocabulary","page":"API Reference","title":"KeemenaPreprocessing.Vocabulary","text":"Vocabulary\n\nImmutable lookup table produced by build_vocabulary that maps between integer token-ids and the string literals that appear in a corpus.\n\nFields\n\nid_to_token_strings::Vector{String}   Position i holds the canonical surface form of token-id i (vocab.id_to_token_strings[id] → \"word\").\ntoken_to_id_map::Dict{String,Int}   Fast reverse mapping from token string to its integer id (vocab.token_to_id_map[\"word\"] → id).   Look-ups fall back to the <UNK> id when the string is absent.\ntoken_frequencies::Vector{Int}   Corpus counts aligned with id_to_token_strings (token_frequencies[id] gives the raw frequency of that token).\nspecial_tokens::Dict{Symbol,Int}   Set of reserved ids for sentinel symbols such as :unk, :pad, :bos, :eos, …   Keys are roles (Symbol); values are the corresponding integer ids.\n\nUsage example\n\nvocab = build_vocabulary(tokens; minimum_token_frequency = 3)\n\n@info \"UNK id:    \" vocab.special_tokens[:unk]\n@info \"«hello» id:\" vocab.token_to_id_map[\"hello\"]\n@info \"id → token:\" vocab.id_to_token_strings[42]\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaPreprocessing._Alignment.alignment_byte_to_word","page":"API Reference","title":"KeemenaPreprocessing._Alignment.alignment_byte_to_word","text":"alignment_byte_to_word(byte_c, word_c) -> CrossMap\n\nConstruct a byte -> word CrossMap that projects each byte index in byte_c onto the word index in word_c that contains it.\n\nPreconditions\n\nbyte_c must have a non-nothing byte_offsets vector   (checked via the private helper _require_offsets).\nword_c must have a non-nothing word_offsets vector.\nBoth corpora must span the same token range   byte_offsets[end] == word_offsets[end]; otherwise an ArgumentError is thrown.\n\nArguments\n\nname type description\nbyte_c Corpus Corpus tokenised at the byte level.\nword_c Corpus Corpus tokenised at the word level.\n\nAlgorithm\n\nRetrieve the sentinel-terminated offset vectors   bo = byte_c.byte_offsets and wo = word_c.word_offsets.\nAllocate b2w :: Vector{Int}(undef, n_bytes) where n_bytes = length(bo) - 1.\nFor each word index w_idx fill the slice wo[w_idx] : wo[w_idx+1]-1 with w_idx, thereby assigning every byte position to the word that begins at wo[w_idx].\nReturn CrossMap(:byte, :word, b2w).\n\nThe output vector has length n_bytes (no sentinel) because every byte token receives one word identifier.\n\nReturns\n\nA CrossMap whose fields are:\n\nsource_level      == :byte\ndestination_level == :word\nalignment         :: Vector{Int}  # length = n_bytes\n\nErrors\n\nArgumentError if either corpus lacks the necessary offsets.\nArgumentError when the overall spans differ.\n\nExample\n\nb2w = alignment_byte_to_word(byte_corpus, word_corpus)\nword_index_of_42nd_byte = b2w.alignment[42]\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaPreprocessing._Assemble.assemble_bundle","page":"API Reference","title":"KeemenaPreprocessing._Assemble.assemble_bundle","text":"assemble_bundle(tokens, offsets, vocab, cfg) -> PreprocessBundle\n\nConvert the token-level artefacts produced by tokenize_and_segment into a minimal yet fully valid PreprocessBundle.  The function\n\nProjects each token to its integer id using vocab; unknown strings are mapped to the :unk special (throws if the vocabulary lacks one).\nPacks the id sequence together with the requested offset tables into a Corpus.\nWraps that corpus and its vocabulary in a LevelBundle whose key is inferred from cfg.tokenizer_name:\ntokenizer_name value level symbol stored\n:byte :byte\n:char :character\n:unicode, :whitespace :word\nFunction Symbol(typeof(fn))\nany other Symbol same symbol\nBuilds a default PipelineMetadata header (PipelineMetadata(cfg, v\"1.0.0\")).\nReturns a PreprocessBundle containing exactly one level, empty alignments, and extras = nothing.\n\nArguments\n\nname type description\ntokens Vector{<:Union{String,UInt8}} Flattened token stream.\noffsets Dict{Symbol,Vector{Int}} Start indices for each recorded level (as returned by tokenize_and_segment).\nvocab Vocabulary Token <-> id mapping (must contain :unk).\ncfg PreprocessConfiguration Determines the level key and special-token requirements.\n\nReturns\n\nPreprocessBundle with\n\nbundle.levels       == Dict(level_key => LevelBundle(corpus, vocab))\nbundle.metadata     == PipelineMetadata(cfg, v\"1.0.0\")\nbundle.alignments   == Dict{Tuple{Symbol,Symbol},CrossMap}()   # empty\nbundle.extras       == nothing\n\nErrors\n\nThrows ArgumentError if vocab lacks the :unk special.\nPropagates any error raised by the inner constructors of Corpus or LevelBundle (e.g. offset inconsistencies).\n\nExample\n\ntokens, offs = tokenize_and_segment(docs, cfg)\nvocab         = build_vocabulary(tokens; cfg = cfg)\nbund          = assemble_bundle(tokens, offs, vocab, cfg)\n\n@info keys(bund.levels)  # (:word,) for whitespace tokenizer\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaPreprocessing._BundleIO.load_preprocess_bundle","page":"API Reference","title":"KeemenaPreprocessing._BundleIO.load_preprocess_bundle","text":"load_preprocess_bundle(path; format = :jld2) -> PreprocessBundle\n\nLoad a previously-saved PreprocessBundle from disk.\n\nThe function currently understands the JLD2 wire-format written by save_preprocess_bundle.  It performs a lightweight header check to ensure the on-disk bundle version is not newer than the library version linked at run-time, helping you avoid silent incompatibilities after package upgrades.\n\nArguments\n\nname type description\npath AbstractString File name (relative or absolute) pointing to the bundle on disk.\nformat Symbol (keyword) Serialization format.  Only :jld2 is accepted—any other value raises an error.\n\nReturns\n\nPreprocessBundle - the exact object originally passed to save_preprocess_bundle, including all levels, alignments, metadata, and extras.\n\nErrors\n\nArgumentError &nbsp;- if path does not exist.\nArgumentError &nbsp;- if format ≠ :jld2.\nErrorException &nbsp;- when the bundle's persisted version is newer than the library's internal _BUNDLE_VERSION, signalling that your local code may be too old to read the file safely.\n\nExample\n\nbund = load_preprocess_bundle(\"artifacts/train_bundle.jld2\")\n\n@info \"levels available: keys(bund.levels))\"\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaPreprocessing._BundleIO.save_preprocess_bundle","page":"API Reference","title":"KeemenaPreprocessing._BundleIO.save_preprocess_bundle","text":"save_preprocess_bundle(bundle, path; format = :jld2, compress = false) -> String\n\nPersist a PreprocessBundle to disk and return the absolute file path written.\n\nCurrently the only supported format is :jld2; an error is raised for any other value.\n\nArguments\n\nname type description\nbundle PreprocessBundle Object produced by preprocess_corpus.\npath AbstractString Destination file name (relative or absolute).  Parent directories are created automatically.\nformat Symbol (keyword) Serialization format. Must be :jld2.\ncompress Bool (keyword) When false (default) the JLD2 file is written without zlib compression; set to false for fastest write speed (default), true adds compression.\n\nFile structure\n\nThe JLD2 file stores three top-level keys\n\nkey value\n\"__bundle_version__\" String denoting the package's internal bundle spec.\n\"__schema_version__\" string(bundle.metadata.schema_version)\n\"bundle\" The full PreprocessBundle instance.\n\nThese headers enable future schema migrations or compatibility checks.\n\nReturns\n\nString - absolute path of the file just written.\n\nExample\n\np = save_preprocess_bundle(bund, \"artifacts/train_bundle.jld2\"; compress = false)\n@info \"bundle saved to p\"\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaPreprocessing._Cleaning.clean_documents","page":"API Reference","title":"KeemenaPreprocessing._Cleaning.clean_documents","text":"clean_documents(docs, cfg) → Vector{String}\n\nApply the text-cleaning stage of the Keemena pipeline to every document in docs according to the options held in cfg (PreprocessConfiguration.   The returned vector has the same length and order as docs.\n\nArguments\n\nname type description\ndocs Vector{String} Raw, unprocessed documents.\ncfg PreprocessConfiguration Cleaning directives (lower-casing, URL replacement, emoji handling, …).\n\nProcessing steps\n\nThe function runs a fixed sequence of transformations, each guarded by the corresponding flag in cfg:\n\nUnicode normalisation normalize_unicode (unicode_normalisation_form).\nHTML stripping strip_html + entity decoding (strip_html_tags, html_entity_decode).\nMarkdown stripping strip_markdown (strip_markdown, preserve_md_code).\nRepeated-character squeezing squeeze_char_runs (squeeze_repeat_chars, max_char_run).\nUnicode confusable mapping normalize_confusables (map_confusables).\nEmoji handling _rewrite_emojis (emoji_handling, emoji_sentinel).\nNumber replacement replace_numbers (replace_numbers, plus the keep_* sub-flags and number_sentinel).\nUnicode-to-ASCII punctuation mapping map_unicode_punctuation (map_unicode_punctuation).\nURL / e-mail replacement replace_urls_emails (replace_urls, replace_emails, url_sentinel, mail_sentinel, keep_url_scheme).\nLower-casing lowercase (lowercase).\nAccent stripping _strip_accents (strip_accents).\nControl-character removal regex replace with _CTRL_RE  (remove_control_characters).\nWhitespace normalisation normalize_whitespace  (normalise_whitespace, remove_zero_width_chars, collapse_spaces,  trim_edges, preserve_newlines).  Falls back to strip when only  trim_edges is requested.\nPunctuation removal regex replace with _PUNCT_RE  (remove_punctuation).\n\nEvery transformation returns a new string; the original input remains unchanged.\n\nReturns\n\nVector{String} — cleaned documents ready for tokenisation.\n\nExample\n\ncfg  = PreprocessConfiguration(strip_html_tags = true,\n                               replace_urls    = true)\nclean = clean_documents([\"Visit https://example.com!\"], cfg)\n@info clean[1]   # -> \"Visit <URL>\"\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaPreprocessing._Tokenization.tokenize_and_segment","page":"API Reference","title":"KeemenaPreprocessing._Tokenization.tokenize_and_segment","text":"\n\n\n\n","category":"function"},{"location":"api/#KeemenaPreprocessing._Vocabulary.build_vocabulary","page":"API Reference","title":"KeemenaPreprocessing._Vocabulary.build_vocabulary","text":"build_vocabulary(tokens::Vector{String}; cfg::PreprocessConfiguration) -> Vocabulary\nbuild_vocabulary(freqs::Dict{String,Int};   cfg::PreprocessConfiguration) -> Vocabulary\nbuild_vocabulary(stream::Channel{Vector{String}};\n                 cfg::PreprocessConfiguration;\n                 chunk_size::Int = 500_000) -> Vocabulary\n\nConstruct a Vocabulary from token data that may be held entirely in memory, pre-counted, or streamed in batches.\n\nMethod overview\n\nVector method - accepts a flat vector of token strings.\nDict method   - accepts a dictionary that maps each token string to its corpus frequency.\nStreaming method - accepts a channel that yields token-vector batches so you can build a vocabulary without ever loading the whole corpus at once.\n\nAll three methods share the same counting, filtering, and ID-assignment logic; they differ only in how token data are supplied.\n\nShared argument\n\ncfg - a PreprocessConfiguration that provides\nminimum_token_frequency\ninitial special_tokens\ndynamic sentence markers when record_sentence_offsets is true.\n\nAdditional arguments\n\ntokens - vector of token strings.\nfreqs  - dictionary from token string to integer frequency.\nstream - channel that produces vectors of token strings.\nchunk_size - number of tokens to buffer before flushing counts (streaming method only).\n\nProcessing steps\n\nSeed specials - copy the special tokens from cfg and insert <BOS> / <EOS> if sentence offsets are recorded.\nCount tokens - accumulate frequencies from the provided data source.\nFilter - discard tokens occurring fewer times than cfg.minimum_token_frequency.\nAssign IDs - assign IDs to specials first (alphabetical order for reproducibility), then to remaining tokens sorted by descending frequency and finally lexicographic order.\nReturn - a deterministic Vocabulary containing token_to_id, id_to_token, and frequencies.\n\nExamples\n\n# From a token vector\ntokens = [\"the\", \"red\", \"fox\", ...]\nvocab  = build_vocabulary(tokens; cfg = config)\n\n# From pre-computed counts\ncounts = Dict(\"the\" => 523_810, \"fox\" => 1_234)\nvocab  = build_vocabulary(counts; cfg = config)\n\n# Streaming large corpora\nch = Channel{Vector{String}}(8) do c\n    for path in corpus_paths\n        put!(c, tokenize(read(path, String)))\n    end\nend\nvocab = build_vocabulary(ch; cfg = config, chunk_size = 100_000)\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaPreprocessing.add_level!-Tuple{PreprocessBundle, Symbol, LevelBundle}","page":"API Reference","title":"KeemenaPreprocessing.add_level!","text":"add_level!(bundle, level, lb) -> PreprocessBundle\n\nMutating helper that inserts a new LevelBundle lb into bundle.levels under key level.  The routine:\n\nGuards against duplicates - throws an error if level already exists.  \nValidates the offsets inside lb.corpus for consistency with the supplied level via validate_offsets.  \nStores the bundle and returns the same bundle instance so the call can be chained.\n\nbe aware that, add_level! modifies its first argument in place; if you require     an immutable bundle keep a copy before calling\n\nArguments\n\nname type description\nbundle PreprocessBundle Target bundle to extend.\nlevel Symbol Identifier for the new segmentation level (e.g. :char, :word).\nlb LevelBundle Data + vocabulary for that level.\n\nReturns\n\nThe same bundle, now containing level => lb.\n\nErrors\n\nArgumentError if a level with the same name already exists.\nPropagates any error raised by validate_offsets when lb.corpus is inconsistent.\n\nExample\n\nchar_bundle = LevelBundle(char_corp, char_vocab)\nadd_level!(bund, :character, char_bundle)\n\n@assert has_level(bund, :character)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.byte_cfg-Tuple{}","page":"API Reference","title":"KeemenaPreprocessing.byte_cfg","text":"byte_cfg(; kwargs...) -> PreprocessConfiguration\n\nShorthand constructor that returns a PreprocessConfiguration pre-configured for byte-level tokenisation.\n\nThe wrapper fixes the following fields\n\ntokenizer_name = :byte\nrecord_byte_offsets      = true\nrecord_character_offsets = false\nrecord_word_offsets      = false\n\nwhile forwarding every other keyword argument to PreprocessConfiguration. Use it when building byte-level language-model corpora but still needing the full flexibility to tweak cleaning, vocabulary, or segmentation options:\n\ncfg = byte_cfg(strip_html_tags = true,\n               minimum_token_frequency = 5)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.get_corpus-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.get_corpus","text":"get_corpus(bundle, level) -> Corpus\n\nRetrieve the Corpus object for segmentation level level from a PreprocessBundle.\n\nThis is equivalent to get_level(bundle, level).corpus and is provided as a convenience helper when you only need the sequence of token-ids and offset tables rather than the whole LevelBundle.\n\nArguments\n\nbundle::PreprocessBundle - bundle produced by preprocess_corpus.\nlevel::Symbol - level identifier such as :byte, :word, :sentence, ...\n\nReturns\n\nThe Corpus stored in the requested level.\n\nErrors\n\nThrows an ArgumentError if the level is not present in bundle (see get_level for details).\n\nExample\n\nword_corp = get_corpus(bund, :word)\n\n# iterate over sentences\nsent_offs = word_corp.sentence_offsets\nfor i in 1:length(sent_offs)-1\n    rng = sent_offs[i] : sent_offs[i+1]-1\n    println(view(word_corp.token_ids, rng))\nend\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.get_level-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.get_level","text":"get_level(bundle, level) → LevelBundle\n\nFetch the LevelBundle associated with segmentation level level from a PreprocessBundle.\n\nArguments\n\nbundle::PreprocessBundle — bundle returned by preprocess_corpus.\nlevel::Symbol — identifier such as :byte, :word, :sentence, ...\n\nReturns\n\nThe requested LevelBundle.\n\nErrors\n\nThrows an ArgumentError when the level is absent, listing all available levels to aid debugging.\n\nExample\n\nword_bundle = get_level(bund, :word)\nprintln(\"vocabulary size: \", length(word_bundle.vocabulary.id_to_token_strings))\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.get_token_ids-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.get_token_ids","text":"get_token_ids(bundle, level) -> Vector{Int}\n\nReturn the vector of token-ids for segmentation level level contained in a PreprocessBundle.\n\nIdentical to   get_corpus(bundle, level).token_ids,   but provided as a convenience helper when you only need the raw id sequence and not the full Corpus object.\n\nArguments\n\nbundle::PreprocessBundle - bundle produced by preprocess_corpus.\nlevel::Symbol - segmentation level identifier (e.g. :byte, :word).\n\nReturns\n\nA Vector{Int} whose length equals the number of tokens at that level.\n\nErrors\n\nThrows an ArgumentError if the requested level is absent (see get_level for details).\n\nExample\n\nword_ids = get_token_ids(bund, :word)\nprintln(\"first ten ids: \", word_ids[1:10])\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.get_vocabulary-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.get_vocabulary","text":"get_vocabulary(bundle, level) -> Vocabulary\n\nReturn the Vocabulary associated with segmentation level level (eg :byte, :word, :sentence) from a given PreprocessBundle\n\nEffectively a shorthand for   get_level(bundle, level).vocabulary\n\nArguments\n\nbundle::PreprocessBundle - Bundle produced by preprocess_corpus\nlevel::Symbol — Level identifier whose vocabulary you need\n\nReturns\n\nThe Vocabulary stored for level\n\nErrors\n\nRaises an ArgumentError if level is not present in bundle (see get_level for details)\n\nExample\n\nvocab = get_vocabulary(bund, :word)\nprintln(\"Top-10 tokens: \", vocab.id_to_token_strings[1:10])\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.has_level-Tuple{PreprocessBundle, Symbol}","page":"API Reference","title":"KeemenaPreprocessing.has_level","text":"has_level(bundle, level) -> Bool\n\nReturn true if the given PreprocessBundle contains a LevelBundle for the segmentation level level (e.g. :byte, :word, :sentence); otherwise return false.\n\nArguments\n\nbundle::PreprocessBundle — bundle to inspect.\nlevel::Symbol            — level identifier to look for.\n\nExample\n\njulia> has_level(bund, :word)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.preprocess_corpus-Tuple{Any, PreprocessConfiguration}","page":"API Reference","title":"KeemenaPreprocessing.preprocess_corpus","text":"preprocess_corpus(sources, cfg; save_to = nothing) - PreprocessBundle\n\nVariant of preprocess_corpus that accepts an already constructed PreprocessConfiguration and therefore bypasses all keyword aliasing and default-override logic.\n\nUse this when you have prepared a configuration object up-front (e.g. loaded from disk, shared across jobs, or customised in a function) and want to run the pipeline with those exact settings.\n\nArguments\n\nname type description\nsources AbstractString, Vector{<:AbstractString}, iterable One or more file paths, URLs, directories (ignored), or in-memory text strings.\ncfg PreprocessConfiguration Fully-specified configuration controlling every cleaning/tokenisation option.\nsave_to String or nothing (default) If non-nothing, the resulting bundle is serialised (e.g. via JLD2) to the given file path and returned; otherwise nothing is written.\n\nPipeline (unchanged)\n\nLoad raw sources.\nClean text based on cfg flags.\nTokenise & segment; record requested offsets.\nBuild vocabulary obeying minimum_token_frequency, special_tokens, ...\nPack everything into a PreprocessBundle.  Optionally persist.\n\nReturns\n\nA PreprocessBundle populated with corpora, vocabularies, alignments, metadata, and (by default) empty extras.\n\nExample\n\ncfg  = PreprocessConfiguration(strip_markdown = true,\n                               tokenizer_name  = :unicode)\n\nbund = preprocess_corpus([\"doc1.txt\", \"doc2.txt\"], cfg;\n                         save_to = \"unicode_bundle.jld2\")\n\nnote: If you do not have a configuration object yet, call the keyword-only version instead:       preprocess_corpus(sources; kwargs...)      which will create a default configuration and apply any overrides you provide.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.preprocess_corpus-Tuple{Any}","page":"API Reference","title":"KeemenaPreprocessing.preprocess_corpus","text":"preprocess_corpus(sources; save_to = nothing,\n                              config = nothing,\n                              kwargs...) -> PreprocessBundle\n\nEnd-to-end convenience wrapper that loads raw texts, cleans them, tokenises, builds a vocabulary, records offsets, and packs the result into a PreprocessBundle.\n\nThe routine can be invoked in two mutually-exclusive ways:\n\nExplicit configuration - supply your own PreprocessConfiguration through the config= keyword.\nAd-hoc keyword overrides - omit config and pass any subset of the configuration keywords directly (e.g. lowercase = false, tokenizer_name = :unicode).   Internally a fresh PreprocessConfiguration(; kwargs...) is created from those overrides plus the documented defaults, so calling preprocess_corpus(sources) with no keywords at all runs the pipeline using the default settings.\n\nnote: Passing both config= and per-field keywords is an error because     it would lead to ambiguous intent.\n\nArguments\n\nname type description\nsources AbstractString, Vector{<:AbstractString}, or iterable Either one or more file paths/URLs that will be read, directories (silently skipped), or in-memory strings treated as raw text.\nsave_to String or nothing (default) If a path is given the resulting bundle is serialised (JLD2) to disk and returned; otherwise nothing is written.\nconfig PreprocessConfiguration or nothing Pre-constructed configuration object.  When nothing (default), a new one is built from kwargs....\nkwargs... see PreprocessConfiguration Per-field overrides that populate a fresh configuration when config is nothing.\n\nPipeline stages\n\nLoading - files/URLs are fetched; directory entries are ignored.\nCleaning - controlled by the configuration's cleaning toggles.\nTokenisation & segmentation - produces token ids and offset tables.\nVocabulary building - applies minimum_token_frequency and inserts special tokens.\nPackaging - returns a PreprocessBundle; if save_to was given, the same bundle is persisted to that path.\n\nReturns\n\nA fully-populated PreprocessBundle.\n\nExamples\n\n# 1. Quick start with defaults\nbund = preprocess_corpus(\"corpus.txt\")\n\n# 2. Fine-grained control via keyword overrides\nbund = preprocess_corpus([\"doc1.txt\", \"doc2.txt\"];\n                         strip_html_tags = true,\n                         tokenizer_name  = :unicode,\n                         minimum_token_frequency = 3)\n\n# 3. Supply a hand-crafted configuration object\ncfg  = PreprocessConfiguration(strip_markdown = true,\n                               record_sentence_offsets = false)\nbund = preprocess_corpus(\"input/\", config = cfg, save_to = \"bundle.jld2\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.preprocess_corpus_streaming-Tuple{Any}","page":"API Reference","title":"KeemenaPreprocessing.preprocess_corpus_streaming","text":"preprocess_corpus_streaming(srcs;\n                            cfg           = PreprocessConfiguration(),\n                            vocab         = nothing,\n                            chunk_tokens  = DEFAULT_CHUNK_TOKENS) -> Channel{PreprocessBundle}\n\nLow-memory, two-pass variant of preprocess_corpus that yields a stream of PreprocessBundle s via a Channel.   Each bundle covers ≈ chunk_tokens worth of tokens, letting you pipeline huge corpora through training code without ever loading the whole dataset into RAM.\n\nWorkflow\n\nVocabulary pass (optional)   If vocab === nothing, the function first computes global token-frequency counts in a constant-memory scan (_streaming_counts) and builds a vocabulary with build_vocabulary(freqs; cfg).   If you already possess a fixed vocabulary (e.g. for fine-tuning), supply it through the vocab keyword to skip this pass.\nChunking iterator   A background task produced by doc_chunk_iterator groups raw source documents into slices whose estimated size does not exceed chunk_tokens.\nPer-chunk pipeline   For every chunk the following steps mirror the standard pipeline:\nclean_documents\ntokenize_and_segment\nassemble_bundle\nbuild_ensure_alignments!\nThe resulting bundle is put! onto the channel.\n\nArguments\n\nname type description\nsrcs iterable of AbstractString File paths, URLs, or raw texts.\ncfg PreprocessConfiguration Cleaning/tokenisation settings (default: fresh object).\nvocab Vocabulary or nothing Pre-existing vocabulary; when nothing it is inferred in pass 1.\nchunk_tokens Int Soft cap on tokens per chunk (default = DEFAULT_CHUNK_TOKENS).\n\nReturns\n\nA channel of type Channel{PreprocessBundle}.   Consume it with foreach, for bundle in ch, or take!(ch).\n\nch = preprocess_corpus_streaming(\"large_corpus/*\";\n                                 cfg = PreprocessConfiguration(strip_html_tags=true),\n                                 chunk_tokens = 250_000)\n\nfor bund in ch                      # streaming training loop\n    update_model!(bund)             # user-defined function\nend\n\nnote: The channel is unbuffered (Inf capacity) so each bundle is produced only     when the consumer is ready, minimising peak memory consumption.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.preprocess_corpus_streaming_chunks-Tuple{Any}","page":"API Reference","title":"KeemenaPreprocessing.preprocess_corpus_streaming_chunks","text":"preprocess_corpus_streaming_chunks(srcs; kwargs...) -> 4Vector{PreprocessBundle}\n\nRun the streaming pipeline once, eagerly consume the channel, and return a Vector whose i-th entry is the PreprocessBundle covering chunk i.\n\nIdentical keyword interface to preprocess_corpus_streaming; all arguments are forwarded unchanged.\n\nUse when you want chunked artefacts (e.g. sharding a massive corpus across GPUs) but prefer a materialised vector instead of an explicit Channel.\n\nbundles = preprocess_corpus_streaming_chunks(\"wiki_xml/*\";\n                                   chunk_tokens = 250_000,\n                                   strip_html_tags = true)\n@info \"produced (length(bundles)) bundles\"\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.preprocess_corpus_streaming_full-Tuple{Any}","page":"API Reference","title":"KeemenaPreprocessing.preprocess_corpus_streaming_full","text":"preprocesscorpusstreaming_full(srcs; kwargs...) -> PreprocessBundle\n\nRun the streaming pipeline, merge every chunk on the fly, and return one single PreprocessBundle that spans the entire corpus.\n\nAll keyword arguments are forwarded to preprocesscorpusstreaming. Throws when chunks were built with incompatible vocabularies\n\nbund = preprocess_corpus_streaming_full([\"en.txt\", \"de.txt\"];\n                              minimum_token_frequency = 5)\nprintln(\"corpus length: \", length(get_token_ids(bund, :word)))\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaPreprocessing.with_extras-Tuple{PreprocessBundle, Any}","page":"API Reference","title":"KeemenaPreprocessing.with_extras","text":"with_extras(original, new_extras) -> PreprocessBundle\n\nCreate a shallow copy of original where only the extras field is replaced by new_extras.  All other components (levels, metadata, alignments) are cloned by reference, so the operation is cheap and the returned bundle remains consistent with the source.\n\nUseful when you have performed post-processing (e.g. dimensionality reduction, cluster assignments, per-document labels) and want to attach the results without mutating the original bundle in place.\n\nArguments\n\nname type description\noriginal PreprocessBundle Bundle produced by preprocess_corpus.\nnew_extras Any Arbitrary payload to store under bundle.extras.\n\nReturns\n\nA new PreprocessBundle{typeof(new_extras)} identical to original except that extras == new_extras.\n\nExample\n\nlabels = collect(kmeans(doc_embeddings, 50).assignments)\nlabeled = with_extras(bund, labels)\n\n@assert labeled.levels === bund.levels         # same reference\n@assert labeled.extras === labels              # updated payload\n\n\n\n\n\n","category":"method"},{"location":"guides/configuration/#Configuration","page":"Configuration","title":"Configuration","text":"PreprocessConfiguration is a single struct that controls every stage of the preprocessing pipeline:\n\nStage What it governs\nCleaning Unicode normalisation, punctuation stripping, URL / e-mail / number replacement, Markdown & HTML removal, emoji handling, repeated-character squeezing, confusable mapping …\nTokenisation Choice of built-in or custom tokenizer; whether to keep zero-length tokens.\nVocabulary Minimum token frequency cutoff; special-token mapping.\nSegmentation Which offset levels (byte, char, word, sentence, paragraph, document) should be recorded.\n\nA brand-new configuration with all defaults is just:\n\nusing KeemenaPreprocessing\ncfg = PreprocessConfiguration()     # ready to go\n\n","category":"section"},{"location":"guides/configuration/#Keyword-reference","page":"Configuration","title":"Keyword reference","text":"Below is an exhaustive table of every keyword accepted by PreprocessConfiguration(; kwargs...).   Arguments are grouped by stage; omit any keyword to keep its default.","category":"section"},{"location":"guides/configuration/#Cleaning-toggles","page":"Configuration","title":"Cleaning toggles","text":"keyword default description\nlowercase true Convert letters to lower-case.\nstrip_accents true Remove combining accent marks.\nremove_control_characters true Drop Unicode Cc / Cf code-points.\nremove_punctuation true Strip punctuation & symbol characters.\nnormalise_whitespace true Collapse consecutive whitespace to a single space.\nremove_zero_width_chars true Remove zero-width joiners, etc.\npreserve_newlines true Keep explicit \\n; needed for paragraph offsets.\ncollapse_spaces true Collapse runs of spaces / tabs.\ntrim_edges true Strip leading / trailing whitespace.","category":"section"},{"location":"guides/configuration/#URL,-e-mail-and-number-replacement","page":"Configuration","title":"URL, e-mail & number replacement","text":"keyword default purpose\nreplace_urls true Replace URLs with url_sentinel.\nreplace_emails true Replace e-mails with mail_sentinel.\nkeep_url_scheme false Preserve http:// / https:// prefix.\nurl_sentinel \"<URL>\" Literal token replacing each URL.\nmail_sentinel \"<EMAIL>\" Literal token replacing each e-mail.\nreplace_numbers false Replace numbers with number_sentinel.\nnumber_sentinel \"<NUM>\" Token used when replacing numbers.\nkeep_number_decimal false Preserve decimal part.\nkeep_number_sign false Preserve + / - sign.\nkeep_number_commas false Preserve thousands separators.","category":"section"},{"location":"guides/configuration/#Mark-up-and-HTML","page":"Configuration","title":"Mark-up & HTML","text":"keyword default description\nstrip_markdown false Remove Markdown formatting.\npreserve_md_code true Keep fenced / inline code while stripping.\nstrip_html_tags false Remove HTML / XML tags.\nhtml_entity_decode true Decode &amp;, &quot;, …","category":"section"},{"location":"guides/configuration/#Emoji-and-Unicode-normalisation","page":"Configuration","title":"Emoji & Unicode normalisation","text":"keyword default description\nemoji_handling :keep :keep, :remove, or :sentinel.\nemoji_sentinel \"<EMOJI>\" Used when emoji_handling == :sentinel.\nsqueeze_repeat_chars false Limit repeated characters (sooooo → sooo).\nmax_char_run 3 Max run length when squeezing.\nmap_confusables false Map visually confusable Unicode chars to ASCII.\nunicode_normalisation_form :none :NFC, :NFD, :NFKC, :NFKD, or :none.\nmap_unicode_punctuation false Replace fancy punctuation with ASCII analogues.","category":"section"},{"location":"guides/configuration/#Tokenisation","page":"Configuration","title":"Tokenisation","text":"keyword default description\ntokenizer_name :whitespace One of TOKENIZERS or a custom f(::String) callable.\npreserve_empty_tokens false Keep zero-length tokens if the tokenizer returns them.","category":"section"},{"location":"guides/configuration/#Vocabulary-construction","page":"Configuration","title":"Vocabulary construction","text":"keyword default purpose\nminimum_token_frequency 1 Tokens below this frequency map to <UNK>.\nspecial_tokens Dict(:unk=>\"<UNK>\", :pad=>\"<PAD>\") Role ⇒ literal token mapping.","category":"section"},{"location":"guides/configuration/#Offset-recording","page":"Configuration","title":"Offset recording","text":"keyword default description\nrecord_byte_offsets false Record byte-level spans.\nrecord_character_offsets false Record Unicode-character offsets.\nrecord_word_offsets true Record word offsets.\nrecord_sentence_offsets true Record sentence offsets.\nrecord_paragraph_offsets false Record paragraph offsets (forces preserve_newlines = true).\nrecord_document_offsets true Record document offsets.\n\n","category":"section"},{"location":"guides/configuration/#Built-in-tokenizers","page":"Configuration","title":"Built-in tokenizers","text":"const TOKENIZERS = (:whitespace, :unicode, :byte, :char)\n\nname behaviour typical use\n:whitespace split(text) on Unicode whitespace Most word-level corpora.\n:unicode Iterate grapheme clusters (eachgrapheme) Languages with complex scripts, emoji, accents.\n:byte Raw UTF-8 bytes (UInt8) Byte-level LLM pre-training.\n:char Individual UTF-8 code-units Character-level models / diagnostics.\n\nYou may pass any callable that returns a Vector{<:AbstractString}:\n\nmytok(text) = split(lowercase(text), r\"[ \\-]+\")\n\ncfg = PreprocessConfiguration(tokenizer_name = mytok)","category":"section"},{"location":"guides/configuration/#Example:-using-WordTokenizers.jl-via-an-adapter","page":"Configuration","title":"Example: using WordTokenizers.jl via an adapter","text":"WordTokenizers.jl exposes many tokenizers, and some return SubString{String} (or otherwise vary token element types). To keep KeemenaPreprocessing outputs stable for downstream pipelines, wrap the tokenizer and normalize to Vector{String}.\n\nusing KeemenaPreprocessing\nimport WordTokenizers\n\nfunction wordtokenizers_nltk_tokenizer(text::AbstractString)::Vector{String}\n    # Normalize element type for stable downstream typing\n    return String.(WordTokenizers.nltk_word_tokenize(text))\nend\n\nconfiguration = PreprocessConfiguration(\n    tokenizer_name = wordtokenizers_nltk_tokenizer,\n)\n\ndocuments = [\"Hello, world! This is a test.\"]\nbundle = preprocess_corpus(documents; config = configuration)\n\nNote: avoid WordTokenizers.set_tokenizer(...) in pipeline code since it changes global behavior. Prefer calling the tokenizer function you want explicitly (as above).\n\n","category":"section"},{"location":"guides/configuration/#Helper:-byte_cfg","page":"Configuration","title":"Helper: byte_cfg","text":"cfg = byte_cfg(strip_html_tags = true,\n               minimum_token_frequency = 5)\n\nbyte_cfg is a thin wrapper that pre-sets   tokenizer_name = :byte, record_byte_offsets = true, and disables char / word offsets.   All other keywords are forwarded unchanged.\n\n","category":"section"},{"location":"guides/configuration/#Examples","page":"Configuration","title":"Examples","text":"### Language-agnostic, emoji-masked corpus\n\ncfg = PreprocessConfiguration(\n          strip_html_tags         = true,\n          emoji_handling          = :sentinel,\n          minimum_token_frequency = 3)\n\nbund = preprocess_corpus(\"multilang_news/*\"; config = cfg)\n\n### Paragraph-level offsets for document classification\n\ncfg = PreprocessConfiguration(\n          record_paragraph_offsets = true,   # auto-enables preserve_newlines\n          tokenizer_name            = :unicode)\n\nbund = preprocess_corpus(\"reports/*.txt\"; config = cfg)\n\n### Extreme byte-level pre-training\n\ncfg = byte_cfg(\n          squeeze_repeat_chars    = true,\n          max_char_run            = 5,\n          minimum_token_frequency = 10)\n\nbund = preprocess_corpus(\"c4_dump/*\"; config = cfg, save_to = \"byte_bundle.jld2\")\n\n","category":"section"},{"location":"guides/configuration/#Notes-and-assertions","page":"Configuration","title":"Notes & assertions","text":"minimum_token_frequency must be ≥ 1.  \ntokenizer_name must be one of TOKENIZERS or a callable.  \nEnabling record_paragraph_offsets = true automatically sets preserve_newlines = true (with a warning).  \nemoji_handling must be :keep, :remove, or :sentinel.  \nunicode_normalisation_form must be :none, :NFC, :NFD, :NFKC, or :NFKD.\n\nInvalid combinations raise AssertionError, so mistakes fail fast during configuration construction rather than deep inside the pipeline.\n\n","category":"section"},{"location":"guides/configuration/#Return-value","page":"Configuration","title":"Return value","text":"PreprocessConfiguration(… ) always yields a fully-populated, immutable struct ready to be stored in bundle metadata or reused across jobs.","category":"section"},{"location":"guides/streaming/#Streaming-Pre-processing-API","page":"Streaming Pipeline","title":"Streaming Pre-processing API","text":"KeemenaPreprocessing streaming pipeline lets you work with corpora that do not fit in RAM.   Internally it runs in two passes:\n\nVocabulary pass - a constant-memory scan to count tokens and build the Vocabulary (skipped when you pass vocab=).\nChunk pass - documents are grouped into slices of ≈ chunk_tokens and each slice becomes a PreprocessBundle.\n\nThree helpers expose the same keywords as preprocess_corpus but differ in how you consume the results:\n\nHelper Returns Best when …\npreprocess_corpus_streaming Channel{PreprocessBundle} You want back-pressure inside a training loop.\npreprocess_corpus_streaming_chunks Vector{PreprocessBundle} You prefer materialised chunks (e.g. GPU sharding).\npreprocess_corpus_streaming_full PreprocessBundle You need one big bundle but can't load the raw corpus at once.\n\n","category":"section"},{"location":"guides/streaming/#1-Stream-through-a-Channel-(more-manual)","page":"Streaming Pipeline","title":"1 - Stream through a Channel (more manual)","text":"cfg = PreprocessConfiguration(tokenizer_name=:unicode,\n                              record_document_offsets=true)\n\nch = preprocess_corpus_streaming(\"data/*\"; cfg, chunk_tokens = 250_000)\n\nfor bund in ch                      # JIT production, O(1 chunk) RAM\n    update_model!(bund)             # your training step\nend\n\nThe channel is unbuffered (Inf capacity) - a new bundle is produced only when the consumer is ready.\n\n","category":"section"},{"location":"guides/streaming/#2-·-Collect-chunks-into-a-vector-(more-automatic)","page":"Streaming Pipeline","title":"2 · Collect chunks into a vector (more automatic)","text":"bundles = preprocess_corpus_streaming_chunks(\"wiki_xml/*\";\n                                             cfg          = cfg,\n                                             chunk_tokens = 250_000)\n\n@info \"produced (length(bundles)) bundles\"\nshuffle!(bundles)      # easy data-parallel sharding\n\nInternally identical to collect(preprocess_corpus_streaming(...)).\n\n","category":"section"},{"location":"guides/streaming/#3-·-Merge-chunks-on-the-fly-(automatic)","page":"Streaming Pipeline","title":"3 · Merge chunks on the fly (automatic)","text":"bundle = preprocess_corpus_streaming_full([\"en.txt\", \"de.txt\"];\n                                        cfg          = cfg,\n                                        chunk_tokens = 50_000,\n                                        minimum_token_frequency = 5)\n\n@info \"corpus length: (length(get_token_ids(bundle, :word)))\"\n\nMerges each chunk into an accumulator in constant memory.  \nVerifies all chunks share the same Vocabulary and cfg.  \nCalls build_ensure_alignments! to regenerate byte/char <-> word maps.\n\n","category":"section"},{"location":"guides/streaming/#Choosing-chunk_tokens","page":"Streaming Pipeline","title":"Choosing chunk_tokens","text":"Corpus size Suggested chunk_tokens\n< 1 M words 10 000 - 20 000\n1-10 M words 20 000 - 100 000\n> 10 M words 100 000 + (benchmark)\n\nAim for 'fits comfortably on GPU' rather than 'largest possible.'\n\n","category":"section"},{"location":"guides/streaming/#Sentinel-conventions","page":"Streaming Pipeline","title":"Sentinel conventions","text":"Offset vectors follow one of two patterns:\n\n0 ... N - leading sentinel 0, trailing N\n1 ... N+1 - leading 1, trailing N+1\n\nThe merge helper recognises both.  For any offset vector it guarantees:\n\nissorted(offsets) == true\nfirst(offsets) in (0, 1)\nlast(offsets)  >= n_tokens\n\n","category":"section"},{"location":"guides/streaming/#Common-pitfalls","page":"Streaming Pipeline","title":"Common pitfalls","text":"Pitfall Remedy\nProducer stalls because channel is not consumed Use foreach or collect-based helpers.\nMixing configs or vocabularies then concatenating by hand Use preprocess_corpus_streaming_full, which throws on mismatch.\nChunk size too small (< 2 k tokens) Causes task-switch overhead; start at 10 k.\nAdding new levels with different sentinel rules Extend the merge the helper sentinel logic and add a test.\n\n","category":"section"},{"location":"guides/streaming/#Helper-signatures-(for-reference)","page":"Streaming Pipeline","title":"Helper signatures (for reference)","text":"preprocess_corpus_streaming_chunks(srcs; kwargs...) -> Vector{PreprocessBundle}\n\npreprocess_corpus_streaming_full(srcs; kwargs...)  -> PreprocessBundle\n\npreprocess_corpus_streaming(srcs;\n    cfg           = PreprocessConfiguration(),\n    vocab         = nothing,\n    chunk_tokens  = DEFAULT_CHUNK_TOKENS\n) -> Channel{PreprocessBundle}\n\nAll keyword arguments are forwarded unchanged.   See PreprocessConfiguration for the full list.","category":"section"},{"location":"guides/streaming/#Benchmarks-(indicative)","page":"Streaming Pipeline","title":"Benchmarks (indicative)","text":"Streaming mode is designed for bounded working memory during preprocessing by producing fixed-size token chunks. It trades throughput for a bounded in-memory chunk bundle.\n\nThe repository contains a small reproducible benchmark script:\n\njulia --project bench/scalability_demo.jl\n\nSetup:\n\ncorpus: ~256 MiB (62 sharded text files built from 2 Project Gutenberg books)\ntokenizer_name: :whitespace\nrecordsentenceoffsets: true\nchunktokens (streaming): 250000\n\nScenario Total tokens Time (s) Total allocations (MiB) In-memory artifact size\npreprocess_corpus (single bundle) 43,037,600 41.77 11,457.84 657.28 MiB bundle\npreprocesscorpusstreaming (consume + discard) 43,037,600 79.67 24,381.64 11.25 MiB (max chunk bundle)\n\nNotes:\n\nTotal allocations is cumulative allocation volume, not peak RSS.\nBundle sizes are from Base.summarysize (approx).\nFirst run includes compilation; run twice to obtain steady-state timings.","category":"section"},{"location":"guides/offsets/#Offset-Vectors-and-Segmentation-Levels","page":"Offsets","title":"Offset Vectors & Segmentation Levels","text":"Keemena stores every corpus as a flat vector of token-ids plus one or more   offset vectors that mark the boundaries of higher-level units   (bytes -> characters -> words -> sentences -> paragraphs -> documents).\n\nUnderstanding these vectors lets you\n\nslice substrings for data augmentation,\nproject annotations between levels,\nand validate or extend the pipeline.\n\n\n\n## Anatomy of an offset vector\n\noffsets = [s0, s1, ..., sn]        # length = n_tokens   or   n_tokens + 1\n                                   # 1-based indices into corpus.token_ids\n\nentry meaning\ns0 leading sentinel - 0 or 1 (optional)\ns1 inclusive start index of token i\nsn trailing sentinel - n_tokens or n_tokens+1 (optional)\n\nstart = offsets[i]\nstop  = offsets[i+1] - 1           # inclusive index range of token i\n\nvalidate_offsets guaranteesissorted(offsets, lt = <)  \nfirst(offsets) in (0, 1) (if a leading sentinel exists)  \nlast(offsets) >= n_tokens  \nlength(offsets) >= n_tokens  When sentinel recording is disabled for a level the vector length equals n_tokens and the sentinel checks are skipped.\n\n","category":"section"},{"location":"guides/offsets/#Sentinel-conventions-by-level","page":"Offsets","title":"Sentinel conventions by level","text":"Level symbol Default sentinel style Typical unit\n:byte [0 ... n] UTF-8 byte\n:character [0 ... n] Unicode scalar\n:word [1 ... n+1] whitespace / tokenizer word\n:sentence [1 ... n+1] heuristic sentence\n:paragraph [1 ... n+1] blank-line span\n:document [1 ... n+1] source document\n\nTrailing sentinels may be either the last token index n_tokens (inclusive style) or n_tokens + 1 (exclusive style).   The streaming merge helper accepts both and deduplicates them, so every offset vector in a merged bundle ends with exactly one sentinel.\n\n\n\n## Mapping symbols -> vectors\n\nKeemena keeps an internal lookup table\n\nKeemenaPreprocessing.LEVEL_TO_OFFSETS_FIELD\n\nthat translates a segmentation symbol to the corresponding field name inside Corpus:\n\nSymbol Corpus field\n:byte :byte_offsets\n:character :character_offsets\n:word :word_offsets\n:sentence :sentence_offsets\n:paragraph :paragraph_offsets\n:document :document_offsets\n\nfield = KeemenaPreprocessing.LEVEL_TO_OFFSETS_FIELD[:sentence]\nsent  = getfield(corpus, field)          # Vector{Int} or `nothing`\n\nAdvanced only   The table is accessible but not exported; ordinary users do not modify it directly.  To register a new level use   add_level!(bundle, :my_level, lb) which both validates offsets and updates the lookup table behind the scenes.\n\n\n\n## Cross-level alignment\n\nWhen two levels share the same span (e.g. bytes & characters) Keemena derives a CrossMap:\n\ncm = alignment_byte_to_word(byte_corp, word_corp)\ndst_word_idx = cm.alignment[src_byte_idx]     # O(1) lookup\n\nbuild_ensure_alignments! automatically adds three canonical maps to every bundle:\n\n(:byte      , :character)\n(:byte      , :word)\n(:character , :word)\n\n\n\n## Practical snippets","category":"section"},{"location":"guides/offsets/#Extract-raw-text-for-the-42-nd-word","page":"Offsets","title":"Extract raw text for the 42-nd word","text":"wc   = bundle.levels[:word].corpus\nspan = wc.word_offsets[42] : wc.word_offsets[43] - 1\nraw  = String(codeunits(bundle.extras.raw_text)[span])","category":"section"},{"location":"guides/offsets/#Sentence-lengths-(words-per-sentence)","page":"Offsets","title":"Sentence lengths (words per sentence)","text":"wc  = bundle.levels[:word].corpus\nsnt = wc.sentence_offsets                       # requires record_sentence_offsets=true\nlengths = diff(snt)                             # Vector{Int}","category":"section"},{"location":"guides/offsets/#Shuffle-paragraph-spans","page":"Offsets","title":"Shuffle paragraph spans","text":"pc = bundle.levels[:paragraph].corpus\nspans = [pc.paragraph_offsets[i] : pc.paragraph_offsets[i+1] - 1\n         for i in 1:length(pc.paragraph_offsets)-1]\nshuffle!(spans)\n\n\n\n## Add a custom level (advanced workflow)\n\n#  build monotone offset vector (leading 1, trailing n+1)\nmy_offs = [1, 8, 15, 22, n_tokens + 1]\n\n#  clone an existing corpus and attach new offsets\ncorpus = deepcopy(bundle.levels[:word].corpus)\nsetfield!(corpus, :my_offsets, my_offs)\n\n#  wrap & insert; add_level! validates and registers lookup entry\nmy_lvl = LevelBundle(corpus,\n                     bundle.levels[:word].vocabulary)\nadd_level!(bundle, :my_level, my_lvl)\n\n","category":"section"},{"location":"guides/offsets/#Troubleshooting","page":"Offsets","title":"Troubleshooting","text":"Symptom Likely cause Fix\n'offsets define k segments but corpus has n tokens' duplicate or missing trailing sentinel regenerate offsets or let streaming merge rebuild them\n'Offsets must be strictly increasing' offsets edited out of order sort or recreate\nAlignment length mismatch corpora built from different cleaned text re-process both levels in the same pipeline\n\nFollowing these rules keeps all built-in helpers—slicing utilities, streaming merge, alignment builders—working seamlessly and lets your custom levels interoperate with the rest of KeemenaPreprocessing.","category":"section"},{"location":"guides/alignment/#Alignment-helpers","page":"Alignment","title":"Alignment helpers","text":"Many downstream tasks need to project annotations or errors between tokenisation levels : eg 'Which word does byte 123 belong to?'. KeemenaPreprocessing encapsulates these look-ups in a small record:\n\nstruct CrossMap\n    source_level      :: Symbol   # :byte, :character, ...\n    destination_level :: Symbol   # :word, ...\n    alignment         :: Vector{Int}   # 1-based destination index per source\nend\n\nFor every source token index i you obtain the corresponding destination token as map.alignment[i].\n\n","category":"section"},{"location":"guides/alignment/#Low-level-constructors","page":"Alignment","title":"Low-level constructors","text":"Function Produces Preconditions\nalignment_byte_to_word(byte_c, word_c) :byte -> :word each corpus has byte_offsets / word_offsets and both share the same span.\nalignment_char_to_word(char_c, word_c) :character -> :word ditto, but via character offsets.\nalignment_byte_to_char(byte_c, char_c) :byte -> :character ditto.\n\nExample:\n\nb2w = alignment_byte_to_word(byte_corpus, word_corpus)\nword_of_42nd_byte = b2w.alignment[42]\n\nErrors:\n\nArgumentError if the required offset vectors are missing or\nthe two corpora cover different spans (byte_offsets[end] != word_offsets[end]).\n\n","category":"section"},{"location":"guides/alignment/#Bundle-level-helpers","page":"Alignment","title":"Bundle-level helpers","text":"","category":"section"},{"location":"guides/alignment/#_ensure_lower_levels!(bundle)","page":"Alignment","title":"_ensure_lower_levels!(bundle)","text":"bundle = _ensure_lower_levels!(bundle)\n\nIf the bundle has a :word level and that word corpus already stores character_offsets and/or byte_offsets, this function:\n\nsynthesises dummy :character / :byte corpora   token-ids are filled with <UNK>,\nadds them as levels (vocabulary = 1-token dummy),\nleaves existing levels untouched,\nreturns the same bundle (mutated in place).\n\nIdempotent: calling it again is a no-op.","category":"section"},{"location":"guides/alignment/#build_alignments!(bundle;-pairs-[(:byte,:word),-...])","page":"Alignment","title":"build_alignments!(bundle; pairs = [(:byte,:word), ...])","text":"Creates the requested CrossMaps iff the corresponding levels exist and the map is not already present.\n\nbuild_alignments!(bundle)          # default three maps\nbuild_alignments!(bundle; pairs=[(:character,:word)])","category":"section"},{"location":"guides/alignment/#build_ensure_alignments!(bundle)","page":"Alignment","title":"build_ensure_alignments!(bundle)","text":"One-stop convenience:\n\nbuild_ensure_alignments!(bundle)\n\nCalls _ensure_lower_levels!,  \nCalls build_alignments! with the default trio,  \nReturns the bundle (mutated).\n\n","category":"section"},{"location":"guides/alignment/#Typical-workflow","page":"Alignment","title":"Typical workflow","text":"bund = preprocess_corpus(\"alice.txt\", config = cfg)\n\n# guarantee byte/char levels + alignments\nbuild_ensure_alignments!(bund)\n\nword_of_first_byte = bund.alignments[(:byte,:word)].alignment[1]\n\nInside the high-level pipelines:\n\npreprocess_corpus creates all three levels + alignments by default.\npreprocess_corpus_streaming / _chunks / _full call build_ensure_alignments! for every chunk (and again after merging), so the resulting bundles are always fully aligned.\n\n","category":"section"},{"location":"guides/alignment/#Sentinel-assumptions","page":"Alignment","title":"Sentinel assumptions","text":"Offset vectors must satisfy:\n\nissorted(offsets) == true\nfirst(offsets) in (0,1)         # leading sentinel\nlast(offsets)  >=  n_tokens     # trailing sentinel\n\nalignment_* functions interpret every index in offsets[i] : offsets[i+1]-1 as belonging to token i.\n\n","category":"section"},{"location":"guides/alignment/#Troubleshooting","page":"Alignment","title":"Troubleshooting","text":"Symptom Likely cause Fix\nArgumentError: byte and word corpora cover different span Word corpus was trimmed during cleaning but byte corpus was not. Re-run the pipeline; byte and word corpora must originate from the same cleaned text.\nKeyError when accessing bundle.alignments[(src,dst)] Map not built (levels missing or function not called). Call build_ensure_alignments!(bundle) or ensure both levels exist before build_alignments!.\nDummy vocabularies contain only <UNK> Expected - lower levels are placeholders used solely for alignment. \n\n","category":"section"},{"location":"guides/alignment/#Helper-signatures-(for-reference)","page":"Alignment","title":"Helper signatures (for reference)","text":"alignment_byte_to_word(byte_c::Corpus, word_c::Corpus)       -> CrossMap\nalignment_char_to_word(char_c::Corpus, word_c::Corpus)       -> CrossMap\nalignment_byte_to_char(byte_c::Corpus, char_c::Corpus)       -> CrossMap\n\n_ensure_lower_levels!(bundle::PreprocessBundle)              -> PreprocessBundle\nbuild_alignments!(bundle::PreprocessBundle; pairs = ...)     -> PreprocessBundle\nbuild_ensure_alignments!(bundle::PreprocessBundle)           -> PreprocessBundle\n\nOnce you have called build_ensure_alignments!, every bundle is guaranteed to contain the canonical :byte -> :character -> :word chain.","category":"section"},{"location":"#KeemenaPreprocessing.jl","page":"Home","title":"KeemenaPreprocessing.jl","text":"KeemenaPreprocessing is a lightweight, fully streaming text-processing pipeline for Julia.  It converts raw text into a compact, serialisable bundle containing\n\ncleaned documents\nflattened token sequences (byte, char, word or custom)\nstart-index offsets for sentences, paragraphs and documents\na deterministic vocabulary with user-defined special tokens\nauxiliary metadata for downstream models\n\nMemory usage stays predictable—even on huge corpora—because every stage can run incrementally in fixed-size chunks.\n\n","category":"section"},{"location":"#Key-features","page":"Home","title":"Key features","text":"Stage Purpose\nCleaning Lower-cases, strips accents, removes control characters, collapses whitespace and can replace URLs, e-mails or numbers with sentinel tokens.\nTokenisation Built-in byte, Unicode-word, whitespace and character tokenisers plus a hook for your own function.\nSegmentation Optional paragraph and sentence splitters driven by regex.\nVocabulary Frequency filtering, minimum counts, user-defined special tokens.\nStreaming mode Process arbitrarily large corpora via channels so nothing ever has to fit entirely in RAM.\nBundles Pack everything into a single JLD2 file with save_preprocess_bundle.\n\nAll stages are driven by one PreprocessConfiguration object, so the same code works for quick prototypes and full production pipelines.\n\n","category":"section"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"A single call runs the entire pipeline—load, clean, tokenise, build a vocabulary, assemble offsets, pack a bundle, and optionally save to disk:\n\nusing KeemenaPreprocessing\n\nbundle = preprocess_corpus(\"corpus/*.txt\";\n                           tokenizer_name = :unicode,            # override defaults\n                           record_sentence_offsets = true,\n                           minimum_token_frequency = 3,\n                           save_to = \"my_bundle.jld2\")           # optional persistence\n\nPrefer a pre-built configuration object? Pass it through config =:\n\ncfg    = PreprocessConfiguration(tokenizer_name = :byte,\n                                 record_byte_offsets = true)\n\nbundle = preprocess_corpus(\"data/raw.txt\"; config = cfg)\n\n","category":"section"},{"location":"#Documentation-map","page":"Home","title":"Documentation map","text":"Configuration : every option in PreprocessConfiguration\nCleaning : rules, sentinel tokens, customisation\nTokenisation : built-in tokenisers and extensibility\nVocabulary : frequency thresholds, specials, determinism\nStreaming : channels, chunk sizes, memory planning\nSaving & loading : JLD2 helpers for long-running jobs\n\nSee the Guides for worked examples  \nFull API in the reference","category":"section"},{"location":"guides/quickstart/#Quick-Start","page":"Quick Start","title":"Quick Start","text":"The preprocess_corpus wrapper lets you go from raw text -> cleaned, tokenised, aligned, and fully-packaged PreprocessBundle in one line.   Its only required argument is sources (strings, file paths, URLs, or any iterable that mixes them). Everything else is optional.\n\nYou pass… What happens\nNo config= and no keywords A fresh PreprocessConfiguration is created with all documented defaults.\nKeyword overrides but no config= A fresh configuration is built from the defaults plus your overrides.\nconfig = cfg object That exact configuration is used; keyword overrides are forbidden (ambiguity).\n\n","category":"section"},{"location":"guides/quickstart/#Minimal-'hello-bundle'-(all-defaults)","page":"Quick Start","title":"Minimal 'hello bundle' (all defaults)","text":"using KeemenaPreprocessing\n\nbund = preprocess_corpus(\"my_corpus.txt\") #one-liner\n@show bund.vocab_size\n\n","category":"section"},{"location":"guides/quickstart/#Single-*in-memory*-string","page":"Quick Start","title":"Single in-memory string","text":"raw = \"\"\"\n      It was a dark stormy night,\n      And we see Sherlock Holmes.\n      \"\"\"\n\nbund = preprocess_corpus(raw) # treats `raw` as a document\n\n","category":"section"},{"location":"guides/quickstart/#Multiple-strings-(small-ad-hoc-corpus)","page":"Quick Start","title":"Multiple strings (small ad-hoc corpus)","text":"docs = [\n    \"Mary had a little lamb.\",\n    \"Humpty-Dumpty sat on a wall.\",\n    \"\"\"\n    Roses are red,\n    violets are blue\n    \"\"\"\n]\n\nbund = preprocess_corpus(docs;\n                         tokenizer_name = :whitespace,\n                         minimum_token_frequency = 2)\n\n","category":"section"},{"location":"guides/quickstart/#Multiple-**file-paths**","page":"Quick Start","title":"Multiple file paths","text":"sources = [\"data/alice.txt\",\n           \"data/time_machine.txt\",\n           \"/var/corpora/news_2024.txt\"]\n\nbund = preprocess_corpus(sources; lowercase = false)\n\nDirectories in sources are silently skipped; mixing paths and raw strings is fine.\n\n","category":"section"},{"location":"guides/quickstart/#Remote-URLs","page":"Quick Start","title":"Remote URLs","text":"urls = [\n    \"https://www.gutenberg.org/files/11/11-0.txt\",   # Alice\n    \"https://www.gutenberg.org/files/35/35-0.txt\"    # Time Machine\n]\n\nbund = preprocess_corpus(urls;\n                         tokenizer_name = :unicode,\n                         record_sentence_offsets = true)\n\n","category":"section"},{"location":"guides/quickstart/#Zero-configuration-**byte-level**-tokenisation","page":"Quick Start","title":"Zero-configuration byte-level tokenisation","text":"cfg  = byte_cfg()                 # shorthand helper\nbund = preprocess_corpus(\"binary_corpus.bin\", cfg)\n\n","category":"section"},{"location":"guides/quickstart/#Saving-and-loading-bundles","page":"Quick Start","title":"Saving and loading bundles","text":"cfg   = PreprocessConfiguration(minimum_token_frequency = 5)\nbund1 = preprocess_corpus(\"my_corpus.txt\";\n                          config  = cfg,\n                          save_to = \"corpus.jld2\")\n\nbund2 = load_preprocess_bundle(\"corpus.jld2\")\n\n","category":"section"},{"location":"guides/quickstart/#Interoperability","page":"Quick Start","title":"Interoperability","text":"using KeemenaPreprocessing\n\n# 1) Load a preprocessed bundle\nbundle = load_preprocess_bundle(\"corpus.jld2\")\n\n# 2) Choose a segmentation level for modeling (e.g., words)\nword_corpus  = get_corpus(bundle, :word)      # -> Corpus\nvocabulary   = bundle.levels[:word].vocabulary\n\n# 3) Get the token ids as a single flat vector (all documents concatenated)\ntoken_ids = word_corpus.token_ids             # Vector{Int32} (or Int)\n\n# 4) Split token ids by document using the document offset vector\n#    (offsets follow the \"[1 ... n+1]\" sentinel style at word-level)\ndocument_offsets = word_corpus.document_offsets\ndocument_ranges = (document_offsets[i]:(document_offsets[i+1]-1)\n                   for i in 1:length(document_offsets)-1)\ndocument_token_views = [view(token_ids, r) for r in document_ranges]\n\n# 5) Debug / data inspection: map a handful of ids back to strings\nfirst_20_strings = map(id -> vocabulary.string(id), token_ids[1:20])\n\n# 6) Word -> raw-text span (useful for highlighting model outputs)\n#    (see Guides -> Offsets for the sentinel convention)\nword_index = 42\nstart_ix   = word_corpus.word_offsets[word_index]\nstop_ix    = word_corpus.word_offsets[word_index + 1] - 1\nraw_span   = String(codeunits(bundle.extras.raw_text)[start_ix:stop_ix])\n\n# 7) Byte -> word (project low-level artifacts back to words)\nbuild_ensure_alignments!(bundle)  # ensure canonical :byte->:word map exists\nbyte_to_word = bundle.alignments[(:byte, :word)].alignment\nword_of_byte_123 = byte_to_word[123]\n\n","category":"section"},{"location":"guides/quickstart/#Alignments-and-CrossMap","page":"Quick Start","title":"Alignments and CrossMap","text":"Every time you call preprocess_corpus (streaming or not) the helper   build_ensure_alignments! adds deterministic mappings between all recorded segmentation levels:\n\nOffset arrays eg bundle.levels[:word].corpus.sentence_offsets.\nCrossMap : sparse look-up tables linking byte -> char -> word -> sentence indices","category":"section"},{"location":"guides/quickstart/#a-Inspecting-offsets","page":"Quick Start","title":"a Inspecting offsets","text":"wc = get_corpus(bund, :word)     # word-level Corpus\n@show wc.sentence_offsets[1:10]  # sentinel-terminated, always sorted","category":"section"},{"location":"guides/quickstart/#Byte-word-mapping-for-a-single-token","page":"Quick Start","title":"Byte -> word mapping for a single token","text":"btw = bund.levels[:word].cross_map        # `CrossMap` object\nbyte_ix = 12345\nword_ix = btw(byte_ix)    # constant-time lookup","category":"section"},{"location":"guides/quickstart/#Convenience-helpers","page":"Quick Start","title":"Convenience helpers","text":"word_ix = alignment_byte_to_word(bund, byte_ix)\nchar_ix = alignment_byte_to_char(bund, byte_ix)\n\nThese helpers are thin wrappers over CrossMap, but keep your code independent of the underlying representation.","category":"section"},{"location":"guides/quickstart/#Working-with-*multiple*-segmentation-levels","page":"Quick Start","title":"Working with multiple segmentation levels","text":"The pipeline can record byte, character, word, sentence, paragraph, and document offsets simultaneously.   Just enable the flags you need in the configuration:\n\nusing KeemenaPreprocessing\n\ncfg = PreprocessConfiguration(\n          tokenizer_name            = :unicode,    # word-ish tokens\n          record_byte_offsets       = true,\n          record_character_offsets  = true,\n          record_word_offsets       = true,\n          record_sentence_offsets   = true,\n          record_paragraph_offsets  = true,\n          record_document_offsets   = true)\n\nbund = preprocess_corpus(\"demo.txt\"; config = cfg)\n\nbyte_corp = get_corpus(bund, :byte)        # each token is UInt8\nchar_corp = get_corpus(bund, :char)        # Unicode code-points\nword_corp = get_corpus(bund, :word)        # words / graphemes\nsent_offs = word_corp.sentence_offsets     # sentinel-terminated\npara_offs = word_corp.paragraph_offsets\ndoc_offs  = word_corp.document_offsets\n\n@show (byte_corp.token_ids[1:10],\n       char_corp.token_ids[1:10],\n       word_corp.token_ids[1:10])\n\nBy default every offset array is sorted and sentinel-terminated (last == n_tokens + 1), so it is safe to searchsortedlast or binary-search into them.\n\n","category":"section"},{"location":"guides/quickstart/#Supplying-a-**custom-tokenizer**-function","page":"Quick Start","title":"Supplying a custom tokenizer function","text":"Any callable f(::AbstractString) -> Vector{String} can replace the built-ins.   Below we split on whitespace and the dash \"‐\" character:\n\nusing KeemenaPreprocessing\n\nfunction dash_whitespace_tok(text::AbstractString)\n    return split(text, r\"[ \\t\\n\\r\\-]+\", keepempty = false)\nend\n\ncfg = PreprocessConfiguration(\n          tokenizer_name           = dash_whitespace_tok,    # <- callable\n          minimum_token_frequency  = 2,\n          record_word_offsets      = true)\n\ndocs  = [\"state-of-the-art models excel\",   # note the dashes\n         \"art-of-war is timeless\"]\n\nbund   = preprocess_corpus(docs; config = cfg)\n\n# Inspect the custom tokenisation\nwc = get_corpus(bund, :word)\n@show map(tid -> wc.vocabulary.string(tid), wc.token_ids)","category":"section"},{"location":"guides/quickstart/#Tips-for-custom-tokenisers","page":"Quick Start","title":"Tips for custom tokenisers","text":"Requirement Guideline\nReturn type Vector{<:AbstractString} (no UInt8).\nNo trimming If you want empty tokens preserved, call with preserve_empty_tokens = true.\nOffsets Only :byte and :char levels need special handling; CrossMap takes care of higher levels automatically.\n\n","category":"section"},{"location":"guides/quickstart/#pitfalls","page":"Quick Start","title":"pitfalls","text":"Pitfall Symptom Fix\nPassing config= and keyword overrides ErrorException: Pass either config= or per-field keywords, not both. Pick one method; never both.\nrecord_paragraph_offsets = true but preserve_newlines = false Warning and paragraphs not recorded. Enable preserve_newlines (done automatically with a warning).\nUnsupported tokenizer_name symbol AssertionError Check TOKENIZERS or supply a callable.\n\n","category":"section"},{"location":"guides/quickstart/#Why-JLD2-(and-when-to-use-something-else)","page":"Quick Start","title":"Why JLD2 (and when to use something else)","text":"KeemenaPreprocessing uses JLD2 for the convenience helpers save_preprocess_bundle and load_preprocess_bundle. JLD2 is a pure-Julia serialization format that can store arbitrary Julia structs efficiently and produces files compatible with the HDF5 spec.\n\nThe PreprocessBundle itself is just a plain Julia object, so you are not locked into JLD2: if your workflow needs memory-mapped arrays, indexed random access, or cross-language IO, you can write the bundle (or just its large arrays) using a different storage backend.","category":"section"}]
}
