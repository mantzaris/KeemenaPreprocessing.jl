var documenterSearchIndex = {"docs":
[{"location":"#KeemenaPreprocessing","page":"Home","title":"KeemenaPreprocessing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for KeemenaPreprocessing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#KeemenaPreprocessing.PreprocessBundle","page":"Home","title":"KeemenaPreprocessing.PreprocessBundle","text":"PreprocessBundle{IdT,OffsetT,ExtraT}\n\nIdT    : unsigned integer type for token ids (e.g. UInt32)\nOffsetT   : integer type for offsets  (e.g. Int or UInt32)\nExtraT : payload supplied by downstream packages (Nothing by default)\n\n\n\n\n\n","category":"type"},{"location":"#KeemenaPreprocessing.PreprocessConfiguration-Tuple{}","page":"Home","title":"KeemenaPreprocessing.PreprocessConfiguration","text":"PreprocessConfiguration(; kwargs...) -> PreprocessConfiguration\n\nBuild a fully-typed, immutable configuration object that controls every step of preprocess_corpus.  All keyword arguments are optional; the defaults shown below reproduce the behaviour of a 'typical' English-language pipeline.\n\nIf you mistype a keyword, or supply an illegal value, the constructor throws an AssertionError or ArgumentError immediately—so your downstream workflow can never run with hidden mistakes.\n\n────────────────────────────────────────────────────────────────────────────── Cleaning options ──────────────── lowercase                 = true &nbsp;&nbsp;→ convert text to lowercase   strip_accents             = true &nbsp;&nbsp;→ remove Unicode accents/diacritics   remove_control_characters = true   remove_punctuation        = true   normalise_whitespace      = true &nbsp;&nbsp;→ collapse runs of ␠, \\t, \\n into one space  \n\n────────────────────────────────────────────────────────────────────────────── Tokenisation ──────────── tokenizer_name            = :whitespace \\| :unicode \\| callable\n\n:whitespace - split(str) on ASCII whitespace.  \n:unicode    - splits on Unicode word-break boundaries (UAX #29).  \nFunction    - any f(::AbstractString)::Vector{String} you supply (e.g. a SentencePiece processor).\n\npreserve_empty_tokens     = false - keep empty strings that may arise from consecutive delimiters.\n\n────────────────────────────────────────────────────────────────────────────── Vocabulary building ─────────────────── minimum_token_frequency   = 1   -> discard tokens with lower frequency special_tokens            = Dict(:unk => \"<UNK>\", :pad => \"<PAD>\")\n\nThe dictionary is copied internally, so later mutation will not affect existing configurations.\n\n────────────────────────────────────────────────────────────────────────────── Segmentation levels to record (booleans) ──────────────────────────────────────── record_character_offsets  = false   record_word_offsets       = true   record_sentence_offsets   = true   record_paragraph_offsets  = true   record_document_offsets   = true  \n\nThese flags request which offset tables should appear in the resulting PreprocessBundle.  After processing you can inspect bundle.levels_present[:sentence] etc. to see which ones were actually populated.\n\n────────────────────────────────────────────────────────────────────────────── Examples ────────\n\nMinimal default config\n\ncfg = PreprocessConfiguration()\n\n#custom Unicode tokenizer and higher frequency cut-off\n\ncfg = PreprocessConfiguration(tokenizer_name          = :unicode,\n                              minimum_token_frequency = 5,\n                              lowercase               = false)\n                              \n#plug-in your own callable tokenizer (passing a function)\n\nunicode_tokenizer(s) = collect(eachmatch(r\"\\b\\p{L}[\\p{L}\\p{Mn}\\p{Pc}\\p{Nd}]*\b\", s)) .|> string\n\ncfg = PreprocessConfiguration(tokenizer_name = unicode_tokenizer,\n                              remove_punctuation = false)\n                              \n#you can pass cfg straight to preprocess_corpus:\n\nbundle = preprocess_corpus(text_files; config = cfg, save_to = \"bundle.jld2\")\n\n\n\n\n\n","category":"method"},{"location":"#KeemenaPreprocessing.preprocess_corpus-Tuple{Any}","page":"Home","title":"KeemenaPreprocessing.preprocess_corpus","text":"preprocess_corpus(sources; kwargs...) -> PreprocessBundle\n\nHigh-level helper that takes raw text (already loaded or file-paths) and returns a fully populated PreprocessBundle.  Its a convenience wrapper around the individual steps\n\n1 load -> 2 clean -> 3 segment & tokenise -> 4 build vocabulary   5 vectorise -> 6 assemble bundle -> 7 (optional) save to disk\n\nkwargs are automatically split into pipeline keywords and configuration keywords:\n\n┌─────────────┬──────────────────────────────────────────────────────────────┐ │ Pipeline    │ save_to, id_type, offset_type, config               │ │ keywords    │ (see below)                                                 │ ├─────────────┼──────────────────────────────────────────────────────────────┤ │ Config      │ Every field of [PreprocessConfiguration] - e.g.           │ │ keywords    │ lowercase, tokenizer_name, minimum_token_frequency, ... │ └─────────────┴──────────────────────────────────────────────────────────────┘\n\n*Pass **either** a full `config = cfg` or individual configuration\nkeywords, not both.*  Mixing the two throws an error so that typos never\nslip through silently.\n\nArguments\n\nsources   : A Vector{String} of either\n\nfile-paths ending in .txt (they are read with UTF-8),\nor in-memory document strings.\n\nPipeline-level keyword arguments\n\nsave_to         :: String | nothing   : If given, the resulting bundle is also persisted to that path   via save_preprocess_bundle.\n\nid_type         :: Type{<:Unsigned} (default UInt32)   offset_type     :: Type{<:Integer}  (default Int = Int64/Int32)   : Integer widths used for token IDs and offset tables.\n\nconfig          :: PreprocessConfiguration   : Supply a ready configuration object instead of per-field keywords.\n\nExamples\n\nusing KeemenaPreprocessing\n\n#one-liner with defaults\nbundle = preprocess_corpus(\"data/corpus.txt\")\n\n#tweak a few cleaning knobs via keywords\nbundle = preprocess_corpus(glob(\"books/*.txt\");\n                           lowercase=false,\n                           tokenizer_name=:unicode,\n                           minimum_token_frequency=5)\n\n#fit once, reuse configuration object\ncfg = PreprocessConfiguration(strip_accents=false,\n                              tokenizer_name=:unicode)\ntrain = preprocess_corpus(train_files; config=cfg, save_to=\"train.jld2\")\ntest  = preprocess_corpus(test_files;  config=cfg)\n\n#returns a PreprocessBundle whose levels_present flags match the record_*_offsets fields requested in the configuration.\n\n\n\n\n\n","category":"method"},{"location":"#KeemenaPreprocessing.with_extras!-Union{Tuple{OffsetT}, Tuple{IdT}, Tuple{PreprocessBundle{IdT, OffsetT}, Any}} where {IdT, OffsetT}","page":"Home","title":"KeemenaPreprocessing.with_extras!","text":"with_extras!(bundle, new_extras; setlevel = nothing) -> new_bundle\n\nReturn a new PreprocessBundle sharing the same corpus & vocab but carrying new_extras.  If setlevel is provided it toggles the corresponding levels_present flag to true.\n\n\n\n\n\n","category":"method"}]
}
